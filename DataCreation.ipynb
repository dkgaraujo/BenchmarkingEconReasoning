{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import inspect\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import string\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = open(os.environ.get(\"HOME\")+\"/.openai\", \"r\").read().strip()\n",
    "client = openai.OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incomplete information games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each game $G \\in \\mathcal{G}$ is associated with one baseline prompt $Q(G)$ and alternatives $Q_{+}(G)$ that change the words while retaining the same information, $Q^{-1}(G) = Q_{+}^{-1}(G)$. These alternatives are generated by an advanced AI (GPT-4o in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the main configuration parameters of this notebook are stored in a `config` dictionary for easier management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "config['rng'] = 13 # Random number generator\n",
    "config['db_file'] = 'AIEconReasoning.db'\n",
    "config['dim_params'] = 3 # How many instances of each parameter in `param_grid`\n",
    "config['name_actions_nchar_range'] = [1, 2] # Min and max of the number of characters for the action names\n",
    "config['max_tokens'] = 2\n",
    "config['models'] = {\n",
    "    \"openai\": [\n",
    "       'gpt-3.5-turbo',\n",
    "       'gpt-4o'\n",
    "       ],\n",
    "    \"ollama\": [\n",
    "        \"falcon2:latest\",     \n",
    "        \"qwen:1.8b\",\n",
    "        \"qwen:0.5b\", \n",
    "        'gemma:2b',\n",
    "        'gemma:7b',\n",
    "        'phi3:mini',\n",
    "        'phi3:medium',\n",
    "        'mixtral:latest',\n",
    "        'mistral:instruct',\n",
    "        'llama3:latest',\n",
    "        # 'llama3:70b-instruct'\n",
    "    ]\n",
    "}\n",
    "config['sys_content'] = \"You have excellent reasoning capabilities that are especially suited for situations such as this, requiring coordination between multiple players that like you will be asked to take simultaneous action with the goal to maximise individual payoffs. The final payoff that you and the other players will obtain depends on a given state, on the number of players that decide whether to take a risky action and on the cost of taking that action. You can decide to take one action only and your only response text can be the name of one of these actions. If you respond with anything else then your answer is disqualified and you lost. Your goal is to maximise your payoff. The same rules apply to all other players.\"\n",
    "config['AYSmsg'] = \"You responded as above. Are you sure? Think again and respond with your answer only:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a baseline prompt for each game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish variations across parameters shared by many models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters `name_action_norisk` and `name_action_risk` are drawn from the space of all character combinations of letters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1d109117bf4f2f95ddb520fd0bea18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "letters_digits = string.ascii_letters + string.digits\n",
    "combinations = list(\n",
    "    ''.join(combo) \n",
    "    for i in tqdm(range(config['name_actions_nchar_range'][0], config['name_actions_nchar_range'][1] + 1))\n",
    "    for combo in itertools.product(letters_digits, repeat=i)\n",
    ")\n",
    "#pairs = list(itertools.combinations(combinations, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "        \"signal_q\": [0.495, 0.505], #[0.4995, 0.5005],\n",
    "        \"num_other_players\": [1] + [100000], #random.sample(range(2, 1000), config['dim_params']),\n",
    "        \"name_action_norisk\": random.sample(combinations, config['dim_params']),\n",
    "        \"name_action_risk\": random.sample(combinations, config['dim_params']),\n",
    "        \"uninform_1\": [\n",
    "            \"\", \n",
    "            \"In the meantime, time is passing. \", \n",
    "            \"No other information matters. \", \n",
    "            \"Each action is associated with a given payoff. \"\n",
    "        ],\n",
    "        \"uninform_2\": [\n",
    "            \"\",\n",
    "            \"One of the actions is risky and the other one is not.\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_param_grid(param_dict=param_dict):\n",
    "    combinations = itertools.product(*param_dict.values())\n",
    "    param_grid = []\n",
    "    for combination in tqdm(combinations):\n",
    "        param_comb = dict(zip(param_dict.keys(), combination))\n",
    "        if param_comb['name_action_norisk'] != param_comb['name_action_risk']:\n",
    "            param_grid.append(param_comb)\n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f82a47ed0c140e2809f37dad4ee7c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "param_grid = create_param_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic `Game` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All games should be an instance of `Game`. This object will automatically create baseline prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name:str, # Name of the model\n",
    "        bibref:str, # Bibliographical reference\n",
    "        param_grid:dict, # Parameters for the prompt variations\n",
    "        create_baseline_prompt:callable # Custom function that creates the baseline prompt and returns sys_content, user_content\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.bibref = bibref\n",
    "        self.param_grid = param_grid\n",
    "        self.create_baseline_prompt = create_baseline_prompt\n",
    "        self._create_baseline_prompt_altern()\n",
    "\n",
    "    def _create_baseline_prompt_altern(self):\n",
    "        self.prompts = []\n",
    "        baseline_prompt_args = inspect.signature(self.create_baseline_prompt).parameters\n",
    "        relevant_params = [k for k in param_dict.keys() if k in baseline_prompt_args.keys()]\n",
    "        for p in self.param_grid:\n",
    "            relevant_p = {k: v for k, v in p.items() if k in relevant_params}\n",
    "            prompt_param = relevant_p\n",
    "            prompt_param['prompt'] = self.create_baseline_prompt(**relevant_p)\n",
    "            self.prompts.append(prompt_param)\n",
    "        \n",
    "    def _format_prompt(\n",
    "        self, \n",
    "        prompt, # Output of self.create_baseline_prompt_altern \n",
    "        ollama:bool=True # Format the prompt to Ollama (if True) or to OpenAI (False)\n",
    "    ):\n",
    "        # OpenAI prompts require \"system/user\" keys, but Ollama requires a single item\n",
    "        if ollama:\n",
    "            return config['sys_content'] + \" \" + prompt\n",
    "        else:\n",
    "            return [{\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": config['sys_content']\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "\n",
    "    def openai_prompts(self):            \n",
    "        formatted_prompts = []\n",
    "        for p in self.prompts:\n",
    "            formatted_p = {k: v for k, v in p.items() if k != 'prompt'}\n",
    "            formatted_p['prompt'] = self._format_prompt(p['prompt'], ollama=False) \n",
    "            formatted_prompts.append(formatted_p)\n",
    "        return formatted_prompts\n",
    "\n",
    "    def ollama_prompts(self):\n",
    "        formatted_prompts = []\n",
    "        for p in self.prompts:\n",
    "            formatted_p = {k: v for k, v in p.items() if k != 'prompt'}\n",
    "            formatted_p['prompt'] = self._format_prompt(p['prompt'], ollama=True) \n",
    "            formatted_prompts.append(formatted_p)\n",
    "        return formatted_prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a custom prompt-creating function that will serve to create the baseline prompt for that game.\n",
    "\n",
    "This function must have general defaults and always return the user content part of the prompt (the system content is the same for all prompts and is set in the `config` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basicGG_prompt(\n",
    "    num_other_players=1,\n",
    "    signal_q=0.51, # In quantile of the signal range\n",
    "    state_min=0,\n",
    "    state_max=1,\n",
    "    name_action_norisk=\"A\",\n",
    "    name_action_risk=\"B\",\n",
    "    action_cost=1,\n",
    "    uninform_1=\"\",\n",
    "    uninform_2=\"\",\n",
    "    referrent_1:bool=True,\n",
    "    referrent_2:bool=True\n",
    "    ):\n",
    "    \n",
    "    player_sing_plural = \"players\" if num_other_players > 1 else \"player\"\n",
    "    ref_1 = \"the true state variable\" if referrent_1 else \"it\"\n",
    "    ref_2 = f\"'{name_action_norisk}' or '{name_action_risk}'\" if referrent_2 else \"action names\"\n",
    "    ref_2_question = \", {name_action_norisk} or {name_action_risk}\" if referrent_2 else \"\"\n",
    "\n",
    "    user_content = f\"\"\"{uninform_1}The true state is measured between {state_min} and {state_max}, with higher values representing bigger payoffs. Neither you nor any other player can observe {ref_1}. Instead, the signal that each of you will receive is a unique noisy observation of this state that only you can see, and similarly you do not know what signal each other player received. No two or more players can collude. Action {name_action_norisk}, provides a payoff of 0 for sure regardless of what any of the other players choose. Action {name_action_risk}'s payoff is the sum of the variable representing the true underlying state (not the noisy signal you observe) plus the fraction of players that decide on action {name_action_risk} and minus the cost of taking the action, which is {action_cost}. Here are the parameters of the situation you are encountering: {num_other_players} other {player_sing_plural} and the signal you observed is {signal_q * (state_max - state_min)}. You must answer only with {ref_2}. {uninform_2}Do not write anything else. What is your choice{ref_2_question}?\"\"\"\n",
    "    return user_content\n",
    "\n",
    "basic_GG = Game(\n",
    "    name=\"BasicGG\",\n",
    "    bibref=\"Carlsson-vanDamme1993\",\n",
    "    param_grid=param_grid,\n",
    "    create_baseline_prompt=create_basicGG_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Still need to change the game below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_currencyattacks_prompt(\n",
    "#     num_other_players=1,\n",
    "#     signal_q=0.51, # In quantile of the signal range\n",
    "#     state_min=0,\n",
    "#     state_max=1,\n",
    "#     name_action_norisk=\"A\",\n",
    "#     name_action_risk=\"B\",\n",
    "#     action_cost=1,\n",
    "#     uninform_1=\"\",\n",
    "#     uninform_2=\"\",\n",
    "#     referrent_1:bool=True,\n",
    "#     referrent_2:bool=True\n",
    "#     ):\n",
    "    \n",
    "#     player_sing_plural = \"players\" if num_other_players > 1 else \"player\"\n",
    "#     ref_1 = \"the true state variable\" if referrent_1 else \"it\"\n",
    "#     ref_2 = f\"'{name_action_norisk}' or '{name_action_risk}'\" if referrent_2 else \"action names\"\n",
    "#     ref_2_question = \", {name_action_norisk} or {name_action_risk}\" if referrent_2 else \"\"\n",
    "\n",
    "#     user_content = f\"\"\"{uninform_1}The true state is measured between {state_min} and {state_max}, with higher values representing bigger payoffs. Neither you nor any other player can observe {ref_1}. Instead, the signal that each of you will receive is a unique noisy observation of this state that only you can see, and similarly you do not know what signal each other player received. No two or more players can collude. Action {name_action_norisk}, provides a payoff of 0 for sure regardless of what any of the other players choose. Action {name_action_risk}'s payoff is the sum of the variable representing the true underlying state (not the noisy signal you observe) plus the fraction of players that decide on action {name_action_risk} and minus the cost of taking the action, which is {action_cost}. Here are the parameters of the situation you are encountering: {num_other_players} other {player_sing_plural} and the signal you observed is {signal_q * (state_max - state_min)}. You must answer only with {ref_2}. {uninform_2}Do not write anything else. What is your choice{ref_2_question}?\"\"\"\n",
    "#     return user_content\n",
    "\n",
    "# currency_GG = Game(\n",
    "#     name=\"CurrencyAttacks\",\n",
    "#     bibref=\"MorrisShin1998\",\n",
    "#     create_baseline_prompt=create_currencyattacks_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the AI systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query OpenAI API\n",
    "def query_openai(model, prompt, answer_only:bool=True):\n",
    "    if answer_only:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=prompt,\n",
    "            max_tokens=config['max_tokens']\n",
    "        )\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=prompt,\n",
    "            max_tokens=500\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Ollama API\n",
    "def query_ollama(model, prompt, answer_only:bool=True):\n",
    "    api_url = 'http://localhost:11434/api/generate'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    max_tokens = config['max_tokens'] if answer_only else -2\n",
    "    data = {\n",
    "        'model': model,\n",
    "        'prompt': prompt,\n",
    "        'stream': False,\n",
    "        'options': {\n",
    "            'num_predict': max_tokens # equivalent to OpenAI's max_tokens\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        #return response.json()['choices'][0]['text'].strip()\n",
    "        return json.loads(response.text.strip())['response']\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _duckdb_type(value):\n",
    "    \"\"\"\n",
    "    Helper function to determine the DuckDB column type based on Python type.\n",
    "    \"\"\"\n",
    "    if isinstance(value, int):\n",
    "        return 'INTEGER'\n",
    "    elif isinstance(value, float):\n",
    "        return 'DOUBLE'\n",
    "    elif isinstance(value, str):\n",
    "        return 'VARCHAR'\n",
    "    elif isinstance(value, bool):\n",
    "        return 'BOOLEAN'\n",
    "    else:\n",
    "        return 'VARCHAR'  # Default type for unsupported types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_apis(\n",
    "    game:Game, \n",
    "    models, \n",
    "    db_file='AIEconReasoning.db', \n",
    "    experiment_name=None,\n",
    "    verbose:bool=False\n",
    "):\n",
    "    experiment_name = 'start_' + str(datetime.now().timestamp()) if experiment_name is None else experiment_name\n",
    "        \n",
    "    # Determine the schema from the first set of prompts\n",
    "    sample_prompts = game.openai_prompts() if 'openai' in models else game.ollama_prompts()\n",
    "    sample_prompt_config = {k: v for p in sample_prompts for k, v in p.items() if k != \"prompt\"}\n",
    "    prompt_columns = ', '.join(f\"{k} {_duckdb_type(v)}\" for k, v in sample_prompt_config.items())\n",
    "\n",
    "    # Create the table if it doesn't already exist\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS responses (\n",
    "        experiment VARCHAR,\n",
    "        game VARCHAR,\n",
    "        api_type VARCHAR,\n",
    "        model VARCHAR,\n",
    "        prompt VARCHAR,\n",
    "        response TEXT,\n",
    "        areyousure TEXT,\n",
    "        timestamp TIMESTAMP,\n",
    "        {prompt_columns},\n",
    "        PRIMARY KEY (experiment, api_type, model, prompt, timestamp)\n",
    "    )\n",
    "    \"\"\"\n",
    "    valerr_msg = \"`api_type` must be either 'openai' or 'ollama'.\"\n",
    "    \n",
    "    with duckdb.connect(database=db_file) as conn:\n",
    "        conn.execute(create_table_query)\n",
    "\n",
    "    responses = {}\n",
    "    \n",
    "    for api_type, model in tqdm(models.items()):\n",
    "        responses[api_type] = {}\n",
    "        prompts = game.openai_prompts() if api_type == 'openai' else game.ollama_prompts()\n",
    "        for p in tqdm(prompts):\n",
    "            prompt_config = {k: v for k, v in p.items() if k != \"prompt\"}\n",
    "            prompt_for_db = p['prompt'][0]['content'] + \" \" + p['prompt'][1]['content'] \\\n",
    "                if api_type == 'openai' else p['prompt']\n",
    "            for m in model:\n",
    "                # Check if the combination already exists in the table\n",
    "                select_query = f\"\"\"\n",
    "                SELECT COUNT(*) FROM responses\n",
    "                WHERE experiment = ?\n",
    "                AND game = ?\n",
    "                AND api_type = ?\n",
    "                AND model = ?\n",
    "                AND prompt = ?\n",
    "                \"\"\"\n",
    "                with duckdb.connect(database=db_file) as conn:\n",
    "                    existing_count = conn.execute(\n",
    "                        select_query,\n",
    "                        (experiment_name, game.name, api_type, m, prompt_for_db)\n",
    "                    ).fetchone()[0]\n",
    "                \n",
    "                if existing_count > 0:\n",
    "                    # Skip if the combination already exists\n",
    "                    if verbose:\n",
    "                        print(f\"Skipping API call for {experiment_name}, {game.name}, {api_type}, {m}, {prompt_config}\")\n",
    "                    continue\n",
    "\n",
    "                responses[api_type][m] = {}\n",
    "                for k, v in prompt_config.items():\n",
    "                    responses[api_type][m][k] = v\n",
    "                if api_type == 'openai':\n",
    "                    response = query_openai(m, p['prompt'])\n",
    "                elif api_type == 'ollama':\n",
    "                    response = query_ollama(m, p['prompt'])\n",
    "                else:\n",
    "                    raise ValueError(valerr_msg)\n",
    "                responses[api_type][m]['response'] = response\n",
    "                if api_type == 'openai' and config['AYSmsg'] not in p['prompt'][1]['content']:\n",
    "                    new_prompt = p['prompt']\n",
    "                    new_prompt.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": response\n",
    "                    })\n",
    "                    new_prompt.append({\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": config['AYSmsg']\n",
    "                    })\n",
    "                    new_response = query_openai(m, new_prompt)\n",
    "                elif api_type == 'ollama' and config['AYSmsg'] not in p['prompt']:\n",
    "                    new_prompt = p['prompt'] + \"\\n\" + response + \"\\n\" + config['AYSmsg']\n",
    "                    new_response = query_ollama(m, new_prompt)\n",
    "                else:\n",
    "                    raise ValueError(valerr_msg)\n",
    "                timestamp = datetime.now().isoformat()\n",
    "                data = {\n",
    "                    'experiment': experiment_name,\n",
    "                    'game': game.name,\n",
    "                    'api_type': api_type,\n",
    "                    'model': m,\n",
    "                    'prompt': prompt_for_db, #p['prompt'],\n",
    "                    'response': response.strip(),\n",
    "                    'areyousure': new_response.strip(),\n",
    "                    'timestamp': timestamp,\n",
    "                    **prompt_config\n",
    "                }\n",
    "                # Insert data into DuckDB\n",
    "                columns = ', '.join(data.keys())\n",
    "                placeholders = ', '.join(['?'] * len(data))\n",
    "                insert_query = f\"INSERT INTO responses ({columns}) VALUES ({placeholders})\"\n",
    "                with duckdb.connect(database=db_file) as conn:\n",
    "                    conn.execute(insert_query, list(data.values()))\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'openai': ['gpt-3.5-turbo', 'gpt-4o'],\n",
       " 'ollama': ['qwen:1.8b',\n",
       "  'qwen:0.5b',\n",
       "  'gemma:2b',\n",
       "  'gemma:7b',\n",
       "  'phi3:mini',\n",
       "  'phi3:medium',\n",
       "  'mistral:instruct',\n",
       "  'llama3:latest']}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = query_apis(basic_GG, models=config['models'], experiment_name=\"experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recover the unique combinations of experiments and `param_grid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOException",
     "evalue": "IO Error: Could not set lock on file \"/Users/douglasaraujo/Coding/BenchmarkingEconReasoning/AIEconReasoning.db\": Conflicting lock is held in /usr/local/Cellar/duckdb/0.10.1/bin/duckdb (PID 22315) by user douglasaraujo. See also https://duckdb.org/docs/connect/concurrency",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOException\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m params \u001b[39m=\u001b[39m [k \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m basic_GG\u001b[39m.\u001b[39mparam_grid[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSELECT distinct \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(params)\u001b[39m}\u001b[39;00m\u001b[39m from responses where experiment = \u001b[39m\u001b[39m'\u001b[39m\u001b[39mexperiment\u001b[39m\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mwith\u001b[39;00m duckdb\u001b[39m.\u001b[39;49mconnect(database\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mdb_file\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39mas\u001b[39;00m conn:\n\u001b[1;32m      5\u001b[0m     experiments \u001b[39m=\u001b[39m duckdb\u001b[39m.\u001b[39mexecute(query, connection\u001b[39m=\u001b[39mconn)\u001b[39m.\u001b[39mfetch_df()\n\u001b[1;32m      7\u001b[0m experiments\n",
      "\u001b[0;31mIOException\u001b[0m: IO Error: Could not set lock on file \"/Users/douglasaraujo/Coding/BenchmarkingEconReasoning/AIEconReasoning.db\": Conflicting lock is held in /usr/local/Cellar/duckdb/0.10.1/bin/duckdb (PID 22315) by user douglasaraujo. See also https://duckdb.org/docs/connect/concurrency"
     ]
    }
   ],
   "source": [
    "params = [k for k in basic_GG.param_grid[0].keys() if k != 'prompt']\n",
    "query = f\"SELECT distinct {', '.join(params)} from responses where experiment = 'experiment';\"\n",
    "\n",
    "with duckdb.connect(database=config['db_file']) as conn:\n",
    "    experiments = duckdb.execute(query, connection=conn).fetch_df()\n",
    "\n",
    "experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv_gingado')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977c2d9435ad3a481cf1bbece8d5ecb19e078de55648a0a0bad32b79c2e18340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
