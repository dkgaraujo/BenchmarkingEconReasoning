{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Measuring the economic reasoning abilities of language models\"\n",
    "author:\n",
    "    name:  \"Douglas K. G. Araujo\"\n",
    "    email: \"douglas.araujo@bis.org\"\n",
    "    affiliations:  \"Bank for International Settlements\"\n",
    "thanks: \"This work represents my opinion and not necessarily that of the BIS.\"\n",
    "abstract: \"Economic reasoning is represented as a state space function.\"\n",
    "format:\n",
    "    pdf:\n",
    "        keep-tex: true\n",
    "        template-partials: \n",
    "            - title.tex\n",
    "        include-in-header:\n",
    "            text: |\n",
    "                \\usepackage[noblocks]\n",
    "                {authblk}\n",
    "                \\renewcommand*{\\Authsep}{, }\n",
    "                \\renewcommand*{\\Authand}{, }\n",
    "                \\renewcommand*{\\Authands}{, }\n",
    "                \\renewcommand\\Affilfont{\\small}\n",
    "                \\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "        number-sections: true\n",
    "        bibliography: \"ref.bib\"\n",
    "execute:\n",
    "    echo: false\n",
    "    warning: false\n",
    "    error: false\n",
    "    cache: true\n",
    "    freeze: auto\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models (LMs), in particular those classified as generative artificial intelligence (gen AI), are finding increasing uses in finance and economics. These models are usually tested for their ability to reason, and seem to do well: for example, OpenAI's GPT-4 boats more than 80% correct results in academic and professional micro- and macroeconomics tests (@achiam2023gpt). Still, even such advanced models can fail miserably. @perez2024testing demonstrate how the same model can correctly solve a logical puzzle requiring reasoning about higher order knowledge, only to fail when irrelevant details are changed. Building on results such as this and other examples that clearly illustrate the limits of rationality assumptions on LMs, this work discusses how to systematically measure *economic* reasoning, combining literatures on economic thought and on computer science about gen AI benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main intuition of this work is to combine a number of building blocks of evaluation. First, adversarial filtering as in HellaSwag. Second, evolving test set based on newly published academic work. Third, perturbations in the spirit of @alzahrani2024benchmarks. And fourth, a mathematical adjustment that takes into account performance across perturbations, penalising results that vary with ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This benchmark evaluation addresses a poignant issue for the economics profession: the lack of publicly available data about how these benchmarks are created and any, and toasted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A model of economic reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result from existing benchmarks is largely, if not completely, directly related to the number of questions correctly answered. However, this measures only the model's ability to answer correctly, *not necessarily* its reasoning capabilities. The latter are part of a latent state space sitting between the input prompt and the answer. More concretely, for an input prompt $X$, which includes a question and any necessary explicit information, the language model is a function $\\mathbf{M}$ that maps it to a given response: $\\mathbf{M} : X \\to y$. In order to show that it is done by reasoning, we need tests (and more specifically, measurements) that convey some information about the inner workings of this function. \n",
    "\n",
    "## Reasoning as an abstract of the input\n",
    "\n",
    "- Input prompt $X$\n",
    "\n",
    "- Transformed into $g(X)$, a state space function that maps it to its abstract fundamentals (similar to manifold learning)\n",
    "\n",
    "- Result based on $g(X)$.\n",
    "\n",
    "## A (very) simple model\n",
    "\n",
    "This section builds on the intuition that in true reasoning, the result should be robust to minute perturbations, ie the model is a constant function over the domain of the input. Formally, both $\\mathbf{M}(X) = y$ and $\\mathbf{M}(X + \\epsilon) = y$ for an infinitesimal $\\epsilon$. This implies the derivative with respect to the input prompt is zero. Using as an approachable example the simplest possible neural network, the logistic regression $\\mathbf{N}(x) = \\sigma(Wx + b)$, such robustness further implies that $\\frac{d\\mathbf{N}}{d x} = \\sigma(Wx + b)(1-\\sigma(Wx + b))W = 0$. Because $W$ cannot be a zero vector in a functioning network that is responsive to its inputs and $\\sigma(Wx + b)(1-\\sigma(Wx + b)) = 0$ has no solution because neither term is 0 or 1 in a sigmoid function with finite inputs, the neural network cannot be a constant function. This extremely simplified example does not bode well for the robustness of results given small perturbations in the input prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning benchmarks in other fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Math\n",
    "\n",
    "- Medical\n",
    "\n",
    "- Biologia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A model of reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section develops a model of reasoning that fits naturally into both natural and artificial LMs. It will serve as the basis for the subsequent analyses and empirical creation of a reasoning benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let a sentence $\\mathbf{S} = (\\theta_1, \\theta_2, \\theta_3, ...)$ be a sequence of tokens-location tuples $\\theta_x = (\\tau, x)$, with each $\\tau \\in \\mathbf{V}$ belonging to a vocabulary $\\mathbf{V}$ and $x \\in \\mathbb{N}^{d_{\\text{model}}}$.[^location] Create a function $\\pi_{i, C} : \\theta, \\mathbf{S} \\to \\{-1, 0, 1\\}$ that maps each token into one of three possibilities: the token's information can be considered a adversarial (-1), irrelevant (0) or relevant (1) with respect to the likelihood of individual (or LM) $i$ uttering another sentence C. For example, take the following quote from the character Barf in the 1987 movie Spaceballs, organised as two sentences \"I'm a mog. Half man, half dog.\" and \"I'm my own best friend.\" With word-level tokenisation, $\\mathbf{S} = \\{(\"\\text{I'm}\", 1), (\"\\text{a}\", 2), (\"\\text{mog}\", 3), (\"\\text{.}\", 4), (\"\\text{Half}\", 5), (\"\\text{man}\", 6), (\"\\text{,}\", 7), (\"\\text{half}\", 8), (\"\\text{dog}\", 9), (\"\\text{.}\", 10)\\}$ and $\\mathbf{C}$ is similarly broken down. This example illustrates that even when there is not a logical connection grounded in truth, tokens in one sentence - even those made up like \"mog\", can have a bearing on the likelihood of tokens appearing in another sentence. This likelihood can differ depending on the location of the token, which also allows for situations where repeteating of a word $\\tau$ is meant to convey different meaning. Another feature of this example is that all $\\pi_{\\text{Barf}, C}(\\theta) = 1 \\forall \\theta \\in \\mathbf{S}$. In the alternative sentence \"I'm a mog. Half man, half dog. I am alive.\", the new component is obviously irrelevant for $\\mathbf{C}$: $\\prod_{x \\in [10, 14]} \\pi_{\\text{Barf}, C}(\\theta_x) = 0$.\n",
    "\n",
    "[^location]: The location is important because it helps define meaning, along with the actual letter (more generally, symbol) content of th token. Note that in this paper, white spaces are abstracted away for expositional simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exposition is important to delve into the reasoning aspect, entirely organised by function $\\pi$. Since $\\pi_{i, C}$ measures how informative a token is for individual $i$'s $\\mathbf{C}$, it constitutes the first aspect of reasoning: to recognise when a token is adversarial, irrelevant or relevant. This step is necessary before the application of any logical rules $\\mathcal{l} \\in \\mathcal{L}$ on the weighted token, $\\pi_{i, C}(\\theta_x) \\theta_x$. The exact underpinnings of these logical rules are beyond the scope of this work - it can be approximated by a possibly non-linear function, $g$. What suffices in this work is to say that reasoning *depends* on correctly classifying the tokens: all relevant tokens must be so identified, lest they be either ignored as the irrelevant ones or taken with the opposite meaning. Similarly, if all relevant tokens are indeed diagnosed correctly but other tokens are also diagnosed as relevant when they are not, then this will cause problems for the correct reasoning. In other words, a first precondition for reasoning is to have a low categorical cross-entropy loss. Intuitively, a pre-condition of reasoning is to correctly interpret the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what determines $\\pi_{i, C}$? A combination of knowledges and rationales or logical relationships.\n",
    "\n",
    "Knowledges: linguistic knowledge, common knowledge and commonsense knowledge\n",
    "\n",
    "Rationales: reasoning from logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with the sentence-level categorical cross-entropy, the individual can establish chains of thought that will finally lead to reasoning. Again, for simplicity, the exact function is not discussed here, other than that it is a potentially simple or complex way to interact. What is important is to add the categorical cross-entropy to the estimation equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning about economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above allows us to estimate reasoning while also breaking down some of its components to better understand them. For example, we can estimate any errors in reasoning into an issue with **interpretation**, **knowledge** and **logical thinking**. The empirical estimation follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each *task* $\\theta \\in \\Theta$ can be asked in various different ways, each one being called a *question* $q \\in \\theta$. Questions vary with respect to their adversarial aspect. Following @alzahrani2024benchmarks, the variations $q$ are:\n",
    "\n",
    "- random choice order\n",
    "\n",
    "- biased choice order\n",
    "\n",
    "- uncommon answer choice symbols\n",
    "\n",
    "- common but unordered answer choice symbols\n",
    "\n",
    "In practice, each task has hundreds of different $q$. The variation in response between the questions within each task will comprise the evaluation of the actual reasoning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, two more tests are added: the changing of irrelevant detail (a la @perez2024testing) and random word repetition as if it were a typo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operational characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- avoid becoming part of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv_gingado')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977c2d9435ad3a481cf1bbece8d5ecb19e078de55648a0a0bad32b79c2e18340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
