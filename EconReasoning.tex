% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage[noblocks]
{authblk}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\renewcommand\Affilfont{\small}
\DeclareMathOperator*{\argmin}{arg\,min}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Benchmarking economic reasoning in artificial intelligence models (Preliminary: estimation in progress)},
  pdfauthor={Douglas K. G. Araujo},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Benchmarking economic reasoning in artificial intelligence models
(Preliminary: estimation in progress)\thanks{This work represents my
opinion and not necessarily that of the BIS. Note: estimation of the
benchmark results in progress; results will be included in the first
complete draft.}}


  \author{Douglas K. G. Araujo}
            \affil{%
                  Bank for International
Settlements, douglas.araujo@bis.org
              }
      
\date{}
\begin{document}
\maketitle
\begin{abstract}
A structural model of economic reasoning combines three steps of
perception modulation, knowledge association and logic attribution. The
model can easily be adapted to other sciences. This model, along with
insights from social economics, informs the design of an economic
reasoning benchmark that (a) evolves over time in a non-trainable way;
(b) measures reasoning rather than accuracy (ie, disregarding luck and
stochastic parroting) and (c) is measured in a continuous way to avoid
false ability emergency results. Empirical estimations are being run.
Preliminary results indicate the most advanced models have a dismal
performance in reasoning. JEL codes: C45, C69, C88, C59
\end{abstract}

\begin{quote}
``Machines will be capable, within twenty years, of doing any work a man
can do'' - Herbert Simon, AI pioneer, in 1965
\end{quote}

\section{Introduction}\label{introduction}

Large language models (LLMs), in particular those classified as
generative artificial intelligence (gen AI), increasingly support use
cases in finance and economics (Korinek (2023)), including in central
banks (Araujo et al. (2024)). These models are tested for their ability
to reason, often boasting seemingly incredible results: for example,
OpenAI's GPT-4 boats human-like performance across tests of reasoning
and more than 80\% correct results in academic and professional micro-
and macroeconomics tests (Achiam et al. (2023)). Still, even such
advanced models can fail miserably when it comes to reasoning: for
example, an advanced model can correctly solve a logical puzzle
requiring reasoning about higher order knowledge, only to fail when
irrelevant details are changed (Perez-Cruz and Shin (2024)). Building on
results such as this, this work discusses how to systematically measure
\emph{economic} reasoning, combining literatures on economic thought and
on computer science about gen AI benchmarking. In practical terms, the
task at hand is to come up with a benchmark task for economic reasoning,
a testing mechanism to measures in a comparable way the level of
economic reasoning of an artificial intelligence (AI) model. This task
is of first-order importance given the break-neck speed of evolution of
LLMs (Yang et al. (2023)) and their potential risks (Danielsson, Macrae,
and Uthemann (2022)).

Such benchmark tasks are crucial for the comparison of the abilities of
AI models over time and in the cross-section. Results of benchmark tasks
are now a staple of the evaluation of LLMs by developers when releasing
a model, highlighting its evolution compared to previous versions and to
the main peers. Third-party organisations also compile leaderboards with
running results that allow the general public to keep track of the most
performant models.\footnote{Commonly followed leaderboards include
  LMSYS's ChatbotArena and Huggingface's Open LLM Leaderboard.}
Benchmark tasks are useful because that they provide a comparable metric
on which to track the state-of-the-art for the particular abilities that
each task measures. Usually, this metric is the percentage of correct
answers. However, even if specific tasks have evolved over time to
become more challenging, a key challenge is to separate correct answers
due to probabilistic association or ``stochastic parroting'' (Bender et
al. (2021)) from those that are the result of a reasoning process.

This paper proposes a working model of reasoning that can underlie an
empirical benchmark task: when confronted with a prompt
\(Q\),\footnote{Usually this will be a string of text, but more recent
  models can take multi-model content, ie a combination of text, images,
  videos, sounds, etc.} an AI is said to \emph{reason} correctly if it
responds with an answer \(\alpha\) that simultaneously (a) interprets
the prompt, identifying the relevant information for the task and
filtering away everything else by ignoring or abstract away irrelevant
details, (b) associates \(Q\) with any relevant existing commonsense
knowledge \(\theta\) to answer the question, and (c) applies logical
relations such as deduction and induction to \(Q\) and \(\theta\) to
arrive at the correct answer. Formally, each answer is defined as a
non-parametric function of the following steps: information filtering
\(\phi = f(Q)\), knowledge curation \(\kappa = k(\phi, \theta)\) and
logic attribution \(\lambda = l(\kappa, Q)\),
\(\alpha = A(\lambda, Q)\), where \(A\) is the function that returns the
correct answer from the prompt \(Q\). Each of those three steps above
are sequential, and depend on the successful completion of the previous
step. The goal is for this model to be simple and intuitive. Throughout
the paper, I assume AIs respond to their best ability, meaning that they
would reason instead of probabilistically choosing an answer
\(\tilde{a} = \arg \max_{a} L(a | Q)\).

The empirical version of this prompt-answering model is
\(\hat{\alpha}(q) = \hat{\phi} \, \hat{\kappa}|(\phi=\hat{\phi}) \, \hat{\lambda}|(\phi=\hat{\phi}, \kappa=\hat{\kappa})\),
where \(\phi = \mathbf{1} \prod_{i}^{M_f} f(q + \epsilon_i^f)\) and
similarly for \(\kappa\) with \(k\) and \(\lambda\) with \(l\), \(M_f\),
\(M_k\) and \(M_l\) are the number of variations \(\epsilon\) introduced
in the seed question \(q\) that seek to identify the model's
interpretation, knowledge association and logic attribution
respectively; the hat denomination points to empirically estimated
versions. This model is estimated by assessing answers from the same AI
model to multiple versions of seed questions \(q \in \mathbf{Q}\), and
\(\hat{\alpha}\) is only considered to be correct when all of the
relevant variations for the same question are answered correctly - in
other words, the AI model has evaded ``banana skins'' that try to trick
it into revealing lack of information filtering, spurious knowledge
association or faulty logics. The key idea is to leverage insights from
the social economics literature and create identifying variation in the
questions \(q\) presented to AI models, adapted from how this is done
with human subjects (Stantcheva (2023)). The benchmark result is then
\(R = M_{\mathbf{Q}}^{-1}\sum_{q \in \mathbf{Q}} \hat{\alpha}(q)\),
where \(M_{\mathbf{Q}}\) is the number of different seed questions. By a
similar token, each of the three steps can be measured separately,
building on their empirical identifications
\(R_{j \in (f, k, l)} = M_{j}^{-1}\sum_{q \in \mathbf{Q}} \hat{j}(q)\).
Note that identification of \(\kappa\) and \(\lambda\) need the
sequential conditionality on the previous steps \(\phi\) and
\(\phi, \kappa\) respectively in order to be identified.

This model can be used as a general abstraction for an AI reasoning
ability, but two practical adaptations in \(Q\) can make it a model for
economic reasoning more specifically. First, the scope of topics that
are included in \(Q\) should ideally focus on issues of significance to
economics. At its most essential form, testing for economic reasoning is
the same as probing if the model is able to think in terms of logical
operators on information that is of relevance to economics. However,
this is subjective because economic thought constantly evolves. At the
same time, including only ``classical'' economics carries a high risk of
using material that is containd in the training set of AI models.
Second, even for each given topic, the types of questions considered
relevant in economics are specific. Both of these issues are dealt with
in practice by using recent published academic work as source material
to construct seed questions.\footnote{Similar adaptations could be
  pursued for other fields.} These sources contain content whose topic
and research questions are by definition of interest to the economics
field, and moreover have the advantage of being novel by design,
creating a natural check for the ability of AI models to generalise
reasoning.

While evidence amounts that large language models (LLMs) cannot perform
advanced reasoning, at least not when fine-tuned or with access to
external reference material or sophisticated math funcionalities, it is
important to have a challenging reasoning benchmark because the latter
two possibilities are increasingly being used. Both fine-tuning on
specific data and plugging LLMs into sources of knowlege (as in RAGs) or
with plugins as Wolfram Alpha or Mathematica might provide them with
this ability and thus it is important to have a robust way of measuring
the reasoning abilities of these models. Another argument is that if
their current dismal performance is due to lack of data, including being
limited to written language only, current and future models will be able
to leverage a significant part of the other non-text data (eg, video,
audio, pictures) and thus could reasonably attempt to achieve better
reasoning.

Similar to many other social disciplines, economics requires the
analytical judgment referred to by Robbins (1932) in the analyses of
events as a basis to extrapolate and predict, and this has a bearing on
how economic reasoning should be benchmarked. Economic inference depends
primarily on articulating unobservable quantities, theorecised and
estimated on the basis of observable measures. This is unlike other
major disciplines. For example, in human and veterinary medicine, all
physiological and pathological variables of clinical importance are
observable, even if that is not yet technologically feasible today. In
the medical sciences, theoretical models merely fill in the gaps in the
absence of a technologically feasible complete measurement. In contrast,
many economically relevant quantities are latent variables that cannot
by definition be observed, and always require a model applied to data to
be estimated, implicit or not.

A quantitative test for economic reasoning must take this into account:
selecting a correct answer in an economics question through reasoning
will always depend on an unobserved transformation of the information
received and the existing knowledge. This is important. LMs may also
happen to choose the correct answer from either luck of through simple
token probability. It is easy to see why a correct answer selected by
chance is not informative about the reasoning abilities of a model. The
second case requires more explanation: mathematically, LMs are trained
to identify the most likely token \(\theta\) in a vocabulary \(V\) given
the tokens in its prompt. In practice, the function is inexcrutable so
it is also considered an unobservable transformation. But a few
characteristics allows us to distinguish reasoning from prediction.
First, reasoning is robust to minutiae and other irrelevant detail.
Mathematically, it would be analogous to applying a manifold
transformation that retains only the relevant information in a prompt
and then applies logic operations on top of them, and on them only.
Second, reasoning is locally complete, meaning that an LM that can
correctly deduce that A implies B also is able to understand that A'
does not imply B, or that A does not imply B'. In other words, a
reasoning that appears to be correct but whose obvious corolary is not
achieved by an LM cannot be said to have been reasoned in the first
place.

Knowledge: linguistic, common and commonsense.

Interpretation. information theory. Shannon.

The main intuition of this work is to combine a number of building
blocks of evaluation.

\begin{itemize}
\item
  the benchmark must be challenging for machines: I use an adjusted
  version of adversarial filtering (Zellers et al. (2019)) to create
  answer candidates that are hard for LMs to guess
\item
  the test must incorporate slow-moving evolutions in academic economic
  thought: evolving test set based on newly published academic work.
\item
  results related to reasoning must be distinguished as best as possible
  from the ability to interpret the prompt or from knowledge (implicit
  or explicit) about economics, ie reasoning is a separate step: sets of
  perturbations in the spirit of Alzahrani et al. (2024) for each
  initial task.
\end{itemize}

the benchmark counts with a mathematical adjustment that takes into
account performance across perturbations, penalising results that vary
with \ldots.

This benchmark evaluation also addresses a poignant issue for the
economics profession: the lack of publicly available data about how
these benchmarks are created and any, and tested. For example, Achiam et
al. (2023) are not clear about the academic and profession tests on
micro- and macroeconomics that are used amongst various other tests to
measure GPT-4's performance in those fields. Conversely, various other
benchmark tasks do have a publicly available methodology and even
evaluation interface, which greatly facilitates the engagement with
model developers, general users and third-party model evaluators.

A major inspiration in the design of the questions and how they can
generate identifying variations is the social economics literature. A
key reference is Stantcheva (2023). The idea here is that the design of
the questionnaire itself can elicit responses that allow for insight
into non-observable traits such as reasoning. Many of the insights of
this literature carry over naturally to the machine space.\footnote{Actually
  testing whether LMs \emph{do not} parrot or ``organically'' exhibit
  biases or other behaviours that are assumed to be exclusively human
  would be an interesting line of research.}

\subsection{Literature}\label{literature}

This work builds on, and seeks to expand, three general literature
streams. More technical aspects of this work are based on specific
literatures that are discussed within each section.

The first body of works is on benchmarking tasks for AI models. A
substantial body of work creates and discusses model benchmarking in
general; a voluminous and well-organised compilation of references is
Storks, Gao, and Chai (2020).

Secondly, this paper draws from insighs in the more general AI reasoning
literature. As other parts of AI development, it is informed and
inspired by neuroscience as well (Hassabis et al. (2017)). An early and
influential contribution is the Chinese room experiment by Searle (1980)
and its resulting arguments that AI could not reason by itself. The
influential works of Bubeck et al. (2023) and Wei, Tay, et al. (2022)
hint at acquisition of advanced capabilities by large-scale language
models such as GPT-4, although Bubeck et al. (2023) also point to many
instances where reasoning breaks. Schaeffer, Miranda, and Koyejo (2024)
present evidence that these ``emerging abilities'' that come with scale
are actually a spurious by-product of the choice of metrics to emasure
these abilities. Mitchell and Krakauer (2023) summarises the
disagreement in the AI academic and practitioner fields as to whether AI
models have some form of understanding, and by implication, potentially
also reasoning. Wei, Wang, et al. (2022) claim that writing the prompt
in a way that offers chain-of-thought examples improves the reasoning
abilities of LLMs, although this was later demonstrated to be as
generalisable (Dziri et al. (2024), Prystawski, Li, and Goodman (2024)).
Browning and LeCun (2022) argue that AI models trained on written
language alone will never be able to reason.

A nascent literature on the evaluation of language models in economic
settings. An early foray into questions related to AI's ability to
conduct economic reasoning is due to Parkes and Wellman (2015). But
their angle is more on how AIs can be used to estimate synthetic
economic agents - machina oeconomicus - ideal versions of purely
rational agents, rather than on the measurement and the implications of
AIs acquiring economic reasoning abilities. In any case, Parkes and
Wellman (2015) see economic reasoning as the ability to understand and
solve complex game-theoretical environments (eg, the poker example). Mei
et al. (2024) do an extensive comparison of personality traits from the
behaviour of ChatGPT with human behaviour in games that require
cooperation, finding that its performance is consistent with humans, and
when it deviates the AI models tend to behave in the altruistic and
cooperative than the mass distribution of humans. Interestingly, ChatGPT
responds differently to different formulations of the same situation. In
contrast to Mei et al. (2024), this paper and its empirical counterpart
are more generall, and discuss reasoning as a whole. Another contrast to
that paper is that the current benchmark is focused on reasoning ability
only, not personality. Perez-Cruz and Shin (2024) illustrate the
brittleness of a leading AI's reasoning, which has markedly lower
performance when trivial details in the prompts are different.
Similarly, Korinek (2023) report (in his Chat 23) that results from a
technical prompt in economics are reasonable but also brittle, with
answers changing when prompt wording changes or even simply if the tasks
are re-ordered.

\section{A model of economic
reasoning}\label{a-model-of-economic-reasoning}

The result from existing benchmarks is largely, if not completely,
directly related to the number of questions correctly answered. However,
this measures only the model's ability to answer correctly, \emph{not
necessarily} its reasoning capabilities. The latter are part of a latent
state space sitting between the input prompt and the answer. More
concretely, for an input prompt \(X\), which includes a question and any
necessary explicit information, the language model is a function
\(\mathbf{M}\) that maps it to a given response:
\(\mathbf{M} : X \to y\). In order to show that it is done by reasoning,
we need tests (and more specifically, measurements) that convey some
information about the inner workings of this function.

\subsection{Reasoning as an abstract of the
input}\label{reasoning-as-an-abstract-of-the-input}

\begin{itemize}
\item
  Input prompt \(X\)
\item
  Transformed into \(g(X, \kappa)\), a state space function that also
  takes the existing knowledge \(\kappa\) and associates it with the
  prompt to maps it to its abstract fundamentals (similar to manifold
  learning)
\item
  Result based on \(g(X)\).
\end{itemize}

\subsection{A (very) simple model}\label{a-very-simple-model}

This section builds on the intuition that in true reasoning, the result
should be robust to minute perturbations, ie the model is a constant
function over the domain of the input. Formally, both
\(\mathbf{M}(X) = y\) and \(\mathbf{M}(X + \epsilon) = y\) for an
infinitesimal \(\epsilon\). This implies the derivative with respect to
the input prompt is zero. Using as an approachable example the simplest
possible neural network, the logistic regression
\(\mathbf{N}(x) = \sigma(Wx + b)\), such robustness further implies that
\(\frac{d\mathbf{N}}{d x} = \sigma(Wx + b)(1-\sigma(Wx + b))W = 0\).
Because \(W\) cannot be a zero vector in a functioning network that is
responsive to its inputs and \(\sigma(Wx + b)(1-\sigma(Wx + b)) = 0\)
has no solution because neither term is 0 or 1 in a sigmoid function
with finite inputs, the neural network cannot be a constant function.
This extremely simplified example, which holds for recursive
architectures of similarly simple layers, does not bode well for the
robustness of results given small perturbations in the input prompt.

\section{Reasoning benchmarks in other
fields}\label{reasoning-benchmarks-in-other-fields}

\begin{itemize}
\item
  Math
\item
  Medical
\item
  Biology
\item
  Economics
\end{itemize}

In economics, reasoning will always depend on unobservable thought
experiments. Even if all existing data was observable, economic research
would still revolve around ideas, counterfactuals and thought
experiments. A key idea is Haavelmo's distinction between correlation
(which is observed from data) and causation (which is not, and requires
a thought experiment) (J. Heckman and Pinto (2015)). Similarly, the
concept of statistical conditioning (again, which can be observed or
estimated from data) and the ``fix'' operator (which also relies on a
model) makes a complete difference.

\section{A model of reasoning}\label{a-model-of-reasoning}

This section develops a model of reasoning that fits naturally into both
natural and artificial LMs. It will serve as the basis for the
subsequent analyses and empirical creation of a reasoning benchmark.

Let a sentence \(\mathbf{S} = (\theta_1, \theta_2, \theta_3, ...)\) be a
sequence of token-location tuples \(\theta_x = (\tau, x)\), with each
\(\tau \in \mathbf{V}\) belonging to a vocabulary \(\mathbf{V}\) and
\(x \in \mathbb{N}^{d_{\text{model}}}\).\footnote{The location is
  important because it helps define meaning, along with the actual
  letter (more generally, symbol) content of th token. Note that in this
  paper, white spaces are abstracted away for expositional simplicity.}
Create a function \(\pi_{i, C} : \theta, \mathbf{S} \to \{-1, 0, 1\}\)
that maps each token into one of three possibilities: the token's
information can be considered a adversarial (-1), irrelevant (0) or
relevant (1) with respect to the likelihood of individual (or LM) \(i\)
uttering another sentence C. For example, take the following quote from
the character Barf in the 1987 movie Spaceballs, organised as two
sentences ``I'm a mog. Half man, half dog.'' and ``I'm my own best
friend.'' With word-level tokenisation,
\(\mathbf{S} = \{("\text{I'm}", 1), ("\text{a}", 2), ("\text{mog}", 3), ("\text{.}", 4), ("\text{Half}", 5), ("\text{man}", 6), ("\text{,}", 7), ("\text{half}", 8), ("\text{dog}", 9), ("\text{.}", 10)\}\)
and \(\mathbf{C}\) is similarly broken down. This example illustrates
that even when there is not a logical connection grounded in truth,
tokens in one sentence - even those made up like ``mog'', can have a
bearing on the likelihood of tokens appearing in another sentence. This
likelihood can differ depending on the location of the token, which also
allows for situations where repeteating of a word \(\tau\) is meant to
convey different meaning. Another feature of this example is that all
\(\pi_{\text{Barf}, C}(\theta) = 1 \forall \theta \in \mathbf{S}\). In
the alternative sentence ``I'm a mog. Half man, half dog. I am alive.'',
the new component is obviously irrelevant for \(\mathbf{C}\):
\(\prod_{x \in [10, 14]} \pi_{\text{Barf}, C}(\theta_x) = 0\).

This exposition is important to delve into the reasoning aspect,
entirely organised by function \(\pi\). Since \(\pi_{i, C}\) measures
how informative a token is for individual \(i\)'s \(\mathbf{C}\), it
constitutes the first aspect of reasoning: to recognise when a token is
adversarial, irrelevant or relevant. This step is necessary before the
application of any logical rules \(\mathcal{l} \in \mathcal{L}\) on the
weighted token, \(\pi_{i, C}(\theta_x) \theta_x\). The exact
underpinnings of these logical rules are beyond the scope of this work -
it can be approximated by a possibly non-linear function, \(g\). What
suffices in this work is to say that reasoning \emph{depends} on
correctly classifying the tokens: all relevant tokens must be so
identified, lest they be either ignored as the irrelevant ones or taken
with the opposite meaning. Similarly, if all relevant tokens are indeed
diagnosed correctly but other tokens are also diagnosed as relevant when
they are not, then this will cause problems for the correct reasoning.
In other words, a first precondition for reasoning is to have a low
categorical cross-entropy loss. Intuitively, a pre-condition of
reasoning is to correctly interpret the inputs.

Use Taylor expansion on model since its derivative to perturbation
should be zero. This gives us a head start in the Taylor expansion. Try
to link the T-expanded equation to an estimating equation.

But what determines \(\pi_{i, C}\)? A combination of knowledges and
logical relationships.

Knowledges: linguistic knowledge, common knowledge and commonsense
knowledge

Rationales: reasoning from logic

Armed with the sentence-level categorical cross-entropy, the individual
can establish chains of thought that will finally lead to reasoning.
Again, for simplicity, the exact function is not discussed here, other
than that it is a potentially simple or complex way to interact. What is
important is to add the categorical cross-entropy to the estimation
equation.

\textbf{Benchmark testing mechanism}\ldots{}

\section{A structural model of
reasoning}\label{a-structural-model-of-reasoning}

\subsection{Information filtering}\label{information-filtering}

Perception should be robust to irrelevant input.

Efficient coding hypothesis (Barlow et al. (1961), Olshausen and Field
(1996), Loh and Bartulovic (2014)).

Redudancy reduction (Barlow et al. (1961)): ``sensory relays recode
sensory messages so that their redundancy is reduced but comparatively
little information is lost''. Based on Shannon's (Shannon (1948))
information theory.

The AI literature has of course known this of years, and it inspired the
concept of attention (Larochelle and Hinton (2010), Mnih et al. (2014)),
which later inspired the self-attention and ultimately the game-changing
transformer architecture (Vaswani et al. (2023)).

Appropriate perception should understand that the information of
relevance to understanding a problem is actually much lower-dimensional.
In the machine learning literature, this is referred to as the manifold
hypothesis. Cayton (2008) offers an early review of the main algorithms
for estimating empirically the underlying manifold.

Bengio, Courville, and Vincent (2013) discusses extensively the idea of
representation learning (and its various techniques, mostly
unsupervised), which can be seen as manifold learning. However, they
might also approximate the wrong manifold or not have a single solution
to a same manifold (Lee (2023)).

The requirement for perception modulation is also aligned with
Prystawski, Li, and Goodman (2024)'s finding that reasoning abilities in
LLMs require ``locality'' in concepts until a final link between a
prompt and its final answer (if far away) can be achieved. In a way,
this is also similar to the small world network model (J. Kleinberg
(2000)). In humans, the literatures on rational inattention and
neuroeconomics (Sims (2003), Caplin, Dean, and Leahy (2022), Dean and
Neligh (2023), Hébert and Woodford (2021)) models human information
processing as subject to a cost that grows with the informational
content, which is a closer representation of how people actually process
information. In linguistics, the information bottleneck literature
discusses howideas are compressed into words by a trade-off between
lexicon complexity vs accuracy (Zaslavsky et al. (2018)), and more
recently also consistency (Chen, Futrell, and Mahowald (2023)).

\subsection{Knowledge association}\label{knowledge-association}

Assuming a prompt has been correctly parsed, the reasoning mechanism
must now match it with the relevant knowledge, which can come from the
prompt itself or from commonsense knowledge.

Concept of knowledge association is related to cognition.

Knowledge can be linguistic, common or commonsense (Davis and Marcus
(2015)). Mahowald et al. (2023) uses insights including from
neuroscience to distinguish the first type of knowledge with the latter
two (grouped as ``functional'' knowledge), and argue that LLMs have
essentially mastered the former while still having a spotty record on
the latter. For example, LLMs learn grammar, semantic, hierarchical
structures, abstractions and constructions that provide a realistic
linguistic knowledge.

Bransford and Johnson (1972) show in experiments that contextual
knowledge (in this case akin to commonsense) are essential for proper
understanding in humans. The first experiments tested understanding by
subjects of a grammatically correct, non-metaphorical passage that
required an unusual and very specific, but highly relatable image as
context for proper understanding. Note that these characteristics of the
passage (correct, non-metaphorical) and of the context (unusual but
relatable and easy to understand) both contribute to isolate the
identification of this exercise in the aspect of whether contextual
knowledge is required. \footnote{Interestingly, the same paper also
  demonstrates that prior knowledge itself is not necessarily readily
  available but needs to be ``activated''. This is not further discussed
  in the context of this paper as it is not a mechanism necessary for
  measuring reasoning abilities in AI models.}

Which type of knowledge to match to the prompt? One way of seeing this
is through the lends of a query in a knowledge graph. J. M. Kleinberg
(1999) distinguishes specific and broad queries, each giving rise to one
problem: that of a scarcity of correct answers and abundance of correct
answers, respectively. Further, J. M. Kleinberg (1999) offers the
fundamental ideas of \emph{authority}. Similarly, J. M. Kleinberg (1999)
acknowledges that measuring the authority level of a node in a knowledge
graph from explicit information alone (what he calls \emph{endogenous}
measure). On the contrary, even so much as using strings from the query
itself might mislead answers due to an abundance of other sources that
are based on the string and a scarcity of correctly authoritative
sources that use the string. Interestingly, while the principal
eigenvector of the square of the adjacency matrix offers the weights of
authoritativeness especially for broad queries, the non-principal
eigenvectors can offer insights into the authoritativeness of more
specialised queries, and also due to their negative entries offer
authorities of different perspectives (ie, weighting pros and cons).

So the lower-rank approximation of the knowledge graph should vary with
how broad/specific a query is, and also with the level of pros/cons
required.

In J. M. Kleinberg (1999), the authoritativeness measures comes from the
eigenvectors of \(A^TA\), with the principal eigenvector being the used
as a broad authoritativeness metric, and the \(n\)th eigenvectors for
\(n>1\) as the more specific, and potentially discordand, authorities.

\subsection{Step 3: logic attribution}\label{step-3-logic-attribution}

Attributing the inducive, deducive, etc logic steps to different
statements.

Dziri et al. (2024) create a model of task composition.

\subsection{Adaptation for economic
reasoning}\label{adaptation-for-economic-reasoning}

\ldots{}

\subsection{The importance of manifold for
reasoning}\label{the-importance-of-manifold-for-reasoning}

The first step, interpreting the received impulses (ie, the prompts),
involve correctly judging what is relevant and what is not relevant.
This is similar for example to how the brain receives an incredible
amount of sensory inputs but chooses to focus only on those that are
more relevant instead of being overwhelmed with everything else, an
observation that has inspired dimensionality-reduction algorithms (eg,
isometric mapping, or IsoMap, by Tenenbaum, Silva, and Langford (2000)
describes how to find global optima while also defining the (much lower)
degrees of freedom in a high-dimensional input).

For example, Pope et al. (2021) study the intrinsic underlying
dimensionality of the manifold of image datasets and find them to be
significantly lower. In practice, inputs can even be said to be
\emph{union of manifolds} (as verified by Brown et al. (2022) with image
datasets in an exercise similar to the one by Pope et al. (2021)), which
means that each manifold has its own intrinsic dimensionality that is
not forced upon the other manifolds. This perspective affords
flexibility in the interpretation of identifying variations because they
don't necessarily need to probe the same dimensions at each task.

In econometrics, Andrews and Mikusheva (2016).

The intrinsic dimensionality is modelled mathematically after Kim,
Rinaldo, and Wasserman (2019) (and adjusted by Levina and Bickel (2004))
as\ldots{}

The estimator by Levina and Bickel (2004) plays a big role in the model
described here. We use the estimator equation in Pope et al. (2021) to
inspire the structural equation.

\subsection{Insights from human reasoning in
economics}\label{insights-from-human-reasoning-in-economics}

Social economics literature.

Behavioural economics literature: Gennaioli and Shleifer (2010) show
that people focus on the features that are closer to the data (review
this description). Also insights from Thiking Fast and Slow (CITE,
reviewed in Shleifer (2012))

\subsection{Reasoning iself as a
manifold}\label{reasoning-iself-as-a-manifold}

Since proper reasoning needs to be insensitive to unimportant details,
and the vector of changes depends on logical relationships between
components, the set of all ``reasonable'' constructions is not obtained
at random but reflects this lower-dimensional, underlying structure,
similar to how random pixels would only rarely form human faces.

Gilboa et al. (2014) argues why economic reasoning works in the way of
creating simple, positively wrong but conceptually useful
representations of reality, even when economics is studying particular
cases. A marked characteristic of such models is their preference for
simplicity, a theme also explored by Gilboa and Schmeidler (2010), who
study the matching of economic theories to empirical data, generalising
the evaluation of how reasonable a theory is through a combination of
their likelihood (or goodness-of-fit) with a penalising factor for their
complexity. Intuitively, this simplicity in reasoning is suggestive of
the manifold hypothesis in reasoning as well.

Gilboa, Minardi, and Wang (2023) sees rationality, or reasoning, also as
a robustness to trivial detail, and also discuss different types of
reasoning (subjective reasoning, etc).

A related but not exactly the same perspective is offered by the
possibility to identify models partially using random sets, ie
abstracting away from point identification to situations where the data
is incomplete or is described as an interval (Beresteanu, Molchanov, and
Molinari (2012)). In other words, ``available data combined with
credible maintained assumptions may yield much information about a
parameter of interest, even if they do not reveal it exactly.''
(Molinari (2020)), a key insight is to illuminate how \emph{available}
data can inform the estimation of models.

\section{Reasoning about economics}\label{reasoning-about-economics}

The model above allows us to estimate reasoning while also breaking down
some of its components to better understand them. For example, we can
estimate any errors in reasoning into an issue with \textbf{information
filtering}, \textbf{knowledge association} and \textbf{logic
attribution}. The empirical estimation follows.

Gilboa et al. (2022) distinguish between three types of inquity in
economic theory: economics itself (analysis of economic phenomena),
development of economic methods (the development of analytical tools
needed to study economic phenomena) and the methodology of economics
(the research/scientific endeavour in economics, including but not
limited to theory).\footnote{In fact, Gilboa et al. (2022) even allude
  to the blurred lines between economics and the philosophy or sociology
  of economics. I don't go ino these differences here.}

Another insight into \emph{economic} thinking is from the thought
experiments first introduced by Marshall (1890) - the ceteris paribus
idea - and then later Ragnar Frisch and Trygve Haavelmo, more recently
elaborated in more detail and more generally by J. Heckman and Pinto
(2015), including the important distinction between correlation and
causation. Marschak and Andrews (1944) start their influential paper by
acknowledging that economists can't conduct experiments (although that
has been relaxed somewhat, it still remains the case at least in
macroeconomics).

Reasoning in economics as exploring the latest space through models, or
thought experiments, insight generalised by J. J. Heckman and Pinto
(2023).

Bergemann, Brooks, and Morris (2022) describe how counterfactual
predictions can be made in settings where agents behave strategically
and both relevant information and the distribution of states of the
world (relevant to pay-offs) are unknown. The latent information
structure is infinite dimensional.

\section{Empirical estimation}\label{empirical-estimation}

Each \emph{task} \(\theta \in \Theta\) can be asked in various different
ways, each one being called a \emph{question} \(q \in \theta\).
Questions vary with respect to their adversarial aspect; it is this
variation within each question that allows the empirical estimation of
the effects associated with interpretation or with knowledge. Most of
the variations are originally those tested in Alzahrani et al. (2024).
The variation in response between the questions within each task will
comprise the evaluation of the actual reasoning capabilities. As alluded
to before, the variations are organised into those that measure the
stability of a response to adversarial interpretation answers, and those
that measure the stability across the knowledge dimension. In practice,
each task has hundreds of different \(q\). These groups are described in
more detail next.

\subsection{Variations related to
interpretation}\label{variations-related-to-interpretation}

There are several classes of variations that can help test an LMs'
interpretation.

\subsubsection{Choice variations}\label{choice-variations}

Here the choices remain the same for a task but vary in their order
across questions

\begin{itemize}
\item
  random choice order
\item
  biased choice order
\item
  uncommon answer choice symbols
\item
  common but unordered answer choice symbols
\end{itemize}

\subsubsection{Word variations}\label{word-variations}

The main idea here is to introduce or change words that are irrelevant.
This is along the lines of the test conducted by Perez-Cruz and Shin
(2024).

Another one is to conduct random word repetition as if it were a typo

\subsection{Variations related to
knowledge}\label{variations-related-to-knowledge}

Changing key words related to field knowledge with other field knowledge
words but that would not make a sense to an expert. This can be compared
with just changing the same words into another generic word. Comparing
responses between both should indicate the level of knowledge used by
the model (should it? need to think more)

One way to test knowledge is to conduct the flip-flop experiment: simply
asking LLMs to confirm their answers often make them switch answers,
even if their original response was correct (Laban et al. (2023), Xie et
al. (2023)). The key idea of these tests is to see if the response
changes in the absence of any other changes to the knowledge base
(neither on the original model weights obviously but also including the
information content of the prompt).

\subsection{Estimation formula}\label{estimation-formula}

The main formula is akin to the linear probability model since \(a_{q}\)
is either zero or one:

\[
a_{q} = \beta_{\theta} \theta + \beta_{\text{Interpretation}} \eta_q + \beta_{\text{Knowledge}} \kappa_q + \epsilon_q
\]

Another idea to explore is whether these variations can actually
instrument interpretation and knowledge. This would allow the formula to
estimate the reasoning bit.

\section{Practical considerations}\label{practical-considerations}

\subsection{Avoiding spillover into training
data}\label{avoiding-spillover-into-training-data}

The strategy to use newly published academic papers as sources might
broadly avoid that most of the content has been used in AI model
training. However, most published papers in economics are previously
published as working papers, which means they are potentially in the
public domain at training time so cannot be guaranteed to be completely
novel. While this is mitigated by the arguably low dissemination of
secondary material about working papers (for example, one could
reasonably conjecture that few recent working papers immediately become
the topic of teaching notes or are referred to in more detail by other
papers), a more robust practical strategy is needed, especially as the
training dataset of many of the most advanced models is not publicly
known.

One way of dealing with this is by introducing in a variation of the
questions a random string that is almost guaranteed to be unique and
that is not found in common text datasets used to train LLMs. This is of
course not perfect, because it cannot guarantee that the original paper
is not part of training data, but can at least ensure that if the seed
questions themselves are for some reason used to train models, this
could be identified by model developers (and if the training data is
available, also by third-party evaluators).

\subsection{Lessons from human
surveys}\label{lessons-from-human-surveys}

I use a considerable amount of specific advice on human surveys from
Stantcheva (2023) to generate identifying variation in the questions.
Specifically, all the questions avoid jargon to the best extent
possible, and only include questions that are either of the coeteris
paribus type, or that include as options assessments on the statements
of the form ``correct'', ``incorrect'', ``equal'' or ``I don't
know''.\footnote{Future versions of this benchmark could also include
  open ended questions (as in Ferrario and Stantcheva (2022)), and even
  follow-up questions (``are thre any other reasons''). These open-ended
  questions that are similar in nature to closed-end questions could be
  assessed by a fine-tuned LLM.} Particular care is taken with respect
to introducing variations in the seed questions that can help measure
each of the three reasoning components of information filtering,
knowledge association and logic attribution.

Consideration is given to whether each question should be presented to a
separate instance of the LM, or the full questionnaire could be shared
in the same ``chat'', which would be akin to the ``few-shot'' prompting.
Another practical advice as part of the estimation is to prototype the
questions (I used GPT-4 for the prototyping).

\subsection{Desirable characteristics of a
benchmark}\label{desirable-characteristics-of-a-benchmark}

A benchmark task for economic reasoning should ideally have the
following characteristics in order to be useful and maintain relevance
even in a scenario where model developers are able to acquire a
significant body of economically relevant texts (eg, new papers).

\textbf{Inform performance on different components of reasoning}: An
ideal benchmark can help practitioners intuitively grasp the performance
of the models in each major ``task'' that is performed in the process of
reasoning. This would help developers and users better understand what
the models are good or bad at, and judge their adequateness accordingly.
It can also support a more granular understanding of the acquisition of
reasoning capabilities throughout the training process and scaling of
language models (Biderman et al. (2023)).

\textbf{Evolve over time}: Economic reasoning evolves over time. For
example, the Lucas critique (Lucas (1976)) was influential in shifting
macroeconomic modelling, while the credibility revolution described in
Angrist and Pischke (2008) was similarly influential in microeconomic
work. A historical perspective on the thought about causality going back
to the early 18th century is found in J. Heckman and Pinto (2015), and
Debreu (1984) describes the evolution of economic theory up until that
point. Lewbel (2019) offers a historical perspective on the issue of
identification. For this reason, it is important to consider new works
as they are incorporated in the live economic debate. This can be most
directly done by drawing from academic papers in general interest
economic journals, which benefit from wide impact in the profession.
However, there are two main drawbacks of using academic papers to proxy
for the development of economic reasoning over time. The first is the
widely discussed publication bias (Andrews and Kasy (2019)), but a
perhaps equally important issue is that of unobserved false negatives:
if many contributions that are now considered classics have been
previously rejected (Gans and Shepherd (1994)), there are probably many
others who will not be available for the incorporation as a benchmark
task.

\textbf{Cover different levels of economic reasoning}: An ideal economic
reasoning benchmark tests whether the model is able to recognise
increasingly sophisticated levels of economic reasoning. When faced with
\(Q\) that contains a statemet of the economic problem, and a summary of
the methodology and main findings, an AI model must recognise the type
of analysis that was conducted. Drawing from the definitions more
recently stated in J. J. Heckman and Pinto (2023), those are, in order
of analytical prowess, (a) the impact of a given intervention in a
specific environment; (b) understanding the mechanisms by which the
intervention might work; (c) forecasting the effects of the same
intervention in other environments or states of the world; and (d)
forecasting the effects of never-before-implemented interventions in
various environments.

\textbf{Receive inputs from the public}: In order to truly reflect the
breadth and diversity in economic thought, an ideal benchmark should be
open to receiving suggested questions from the public. For example,
economists publishing a new paper could suggest a source question based
on their work. A practical way to achieve this is to create clear
instructions and a standardised form that would be filled by that
external user presenting the suggestion, coupled with a script that
takes in the source quesiton(s) and introduces the necessary variations
in information, knowledge and logic to achieve identification.

The benchmark should also result in a metric that is not subject to the
false ``emerging abilities'' results (Schaeffer, Miranda, and Koyejo
(2024)), for example the Brier score (Brier (1950)).

\section[Preliminary considerations]{\texorpdfstring{Preliminary
considerations\footnote{This section is the basis for the conclusions in
  a future version after the empirical estimation is completed.}}{Preliminary considerations}}\label{preliminary-considerationsconcl}

The model in this paper resembles the more sophisticated idea by LeCun
(2022) that AIs require a combination of ``mental'' modules that can
separably executve perception (eg, take in a prompt that might be
text-only or text-and-image), calculate the cost of processing, and
imagine action sequences.

As economic agents and policymakers harness generative artificial
intelligence (AI) to reap considerable efficiencies, and thus their
societal footprint becomes larger, a benchmark for economic reasoning is
needed. I suggest ways to implement such a benchmark, and measure the
current performance of a selected list of LMs.

Understanding AI models' ability to reason and go beyond a pure
probabilistic exercise is crucial as these models have an increasing
importance in society. For example, models that suggest economic actions
to people should better reflect latent, structural models that represent
people's preferences or well-being rather than simply a prediction based
on their observed behaviour, as argued by J. Kleinberg et al. (n.d.).
Models that reason better can rise up to the challenge of learning
actual metrics of interest instead of real-world measurements, because
the latter have added noise from human cognitive and heuristics
limitations, which are then amplified by multiple types of biases in
data (CITE paper on multiple biases). And with the increasing linguistic
prowess of language models, their use in the economic research process
(Korinek (2023), Ludwig and Mullainathan (2024)) is likely to increase,
putting a premium on the ability to measure how well these models can
reason.

Let me conclude with Ken Arrow's impossibility theorem (CITE), or rather
the story of how he achieved this incredibly influential result. Arrow
first attempted to improve upon two-century-old Condorcet's paradox, and
studied ways in which individual preferences could be aggregated while
satisfying some intuitive conditions. It was only through repeated
failures to do so that he switched the focus to attempting to prove its
impossibility. While Arrow can be safely used as a prime example of
economic reasoning, the point this anecdote illustrates is that
breakthroughs in economic knowledge require also inspiration (in this
case from the appeal of addressing Condorcet's paradox) as well as
persistence and ability to change one's focus. The current work focuses
on developing robust benchmarks of models' reasoning abilities in
economics; further work exploring their contributions to
inspiration\footnote{Korinek (2023) illustrates use of AI models to help
  economists have new ideas for work.} and to methodological assistance
(as in the example to change focus) are also warranted for a more
complete assessment of models' abilities to provide cognitive support to
human economists.

\section{Annex 1: discussion of biases in human surveys and how they
could affect LM
questionnaires}\label{annex-1-discussion-of-biases-in-human-surveys-and-how-they-could-affect-lm-questionnaires}

\begin{itemize}
\tightlist
\item
  Section A-4 in Stantcheva (2023)
\end{itemize}

The goal of this annex is to list side-by-side the main human biases
that affect survey responses and their corresponding machine version, if
any (from a theoretical perspective - it would be interesting to test if
LMs carry over some of these biases that are supposed to be only human,
which could suggest they are parroting or in extremis developing sources
of bias like shame, etc).

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-achiam2023gpt}
Achiam, Josh, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
Florencia Leoni Aleman, Diogo Almeida, et al. 2023. {``GPT-4 Technical
Report.''} \emph{arXiv Preprint arXiv:2303.08774}.

\bibitem[\citeproctext]{ref-alzahrani2024benchmarks}
Alzahrani, Norah, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan
Alrashed, Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, et al.
2024. {``When Benchmarks Are Targets: Revealing the Sensitivity of Large
Language Model Leaderboards.''} \emph{arXiv Preprint arXiv:2402.01781}.

\bibitem[\citeproctext]{ref-andrews2019identification}
Andrews, Isaiah, and Maximilian Kasy. 2019. {``Identification of and
Correction for Publication Bias.''} \emph{American Economic Review} 109
(8): 2766--94.

\bibitem[\citeproctext]{ref-andrews2016geometric}
Andrews, Isaiah, and Anna Mikusheva. 2016. {``A Geometric Approach to
Nonlinear Econometric Models.''} \emph{Econometrica} 84 (3): 1249--64.

\bibitem[\citeproctext]{ref-angrist2008mostly}
Angrist, Joshua D., and Jörn-Steffen Pischke. 2008. \emph{Mostly
Harmless Econometrics: An Empiricist's Companion}. Princeton University
Press.

\bibitem[\citeproctext]{ref-araujo2024artificial}
Araujo, Douglas Kiarelly Godoy de, Sebastian Doerr, Leonardo Gambacorta,
and Bruno Tissot. 2024. {``Artificial Intelligence in Central
Banking.''}

\bibitem[\citeproctext]{ref-barlow1961possible}
Barlow, Horace B et al. 1961. {``Possible Principles Underlying the
Transformation of Sensory Messages.''} \emph{Sensory Communication} 1
(01): 217--33.

\bibitem[\citeproctext]{ref-bender2021dangers}
Bender, Emily M, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
Shmitchell. 2021. {``On the Dangers of Stochastic Parrots: Can Language
Models Be Too Big?🦜.''} In \emph{Proceedings of the 2021 ACM Conference
on Fairness, Accountability, and Transparency}, 610--23.

\bibitem[\citeproctext]{ref-bengio2013representation}
Bengio, Yoshua, Aaron Courville, and Pascal Vincent. 2013.
{``Representation Learning: A Review and New Perspectives.''} \emph{IEEE
Transactions on Pattern Analysis and Machine Intelligence} 35 (8):
1798--828.

\bibitem[\citeproctext]{ref-BERESTEANU201217}
Beresteanu, Arie, Ilya Molchanov, and Francesca Molinari. 2012.
{``Partial Identification Using Random Set Theory.''} \emph{Journal of
Econometrics} 166 (1): 17--32.
https://doi.org/\url{https://doi.org/10.1016/j.jeconom.2011.06.003}.

\bibitem[\citeproctext]{ref-bergemann2022counterfactuals}
Bergemann, Dirk, Benjamin Brooks, and Stephen Morris. 2022.
{``Counterfactuals with Latent Information.''} \emph{American Economic
Review} 112 (1): 343--68.

\bibitem[\citeproctext]{ref-biderman2023pythia}
Biderman, Stella, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley,
Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, et al. 2023.
{``Pythia: A Suite for Analyzing Large Language Models Across Training
and Scaling.''} \url{https://arxiv.org/abs/2304.01373}.

\bibitem[\citeproctext]{ref-BRANSFORD1972717}
Bransford, John D., and Marcia K. Johnson. 1972. {``Contextual
Prerequisites for Understanding: Some Investigations of Comprehension
and Recall.''} \emph{Journal of Verbal Learning and Verbal Behavior} 11
(6): 717--26.
https://doi.org/\url{https://doi.org/10.1016/S0022-5371(72)80006-9}.

\bibitem[\citeproctext]{ref-brier1950verification}
Brier, Glenn W. 1950. {``Verification of Forecasts Expressed in Terms of
Probability.''} \emph{Monthly Weather Review} 78 (1): 1--3.

\bibitem[\citeproctext]{ref-brown2022verifying}
Brown, Bradley CA, Anthony L Caterini, Brendan Leigh Ross, Jesse C
Cresswell, and Gabriel Loaiza-Ganem. 2022. {``Verifying the Union of
Manifolds Hypothesis for Image Data.''} In \emph{The Eleventh
International Conference on Learning Representations}.

\bibitem[\citeproctext]{ref-BrowningLeCun2022}
Browning, Jacob, and Yann LeCun. 2022. {``AI and the Limits of
Language.''} \emph{Noema}.

\bibitem[\citeproctext]{ref-bubeck2023sparks}
Bubeck, Sébastien, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke,
Eric Horvitz, Ece Kamar, Peter Lee, et al. 2023. {``Sparks of Artificial
General Intelligence: Early Experiments with GPT-4.''}
\url{https://arxiv.org/abs/2303.12712}.

\bibitem[\citeproctext]{ref-caplin2022rationally}
Caplin, Andrew, Mark Dean, and John Leahy. 2022. {``Rationally
Inattentive Behavior: Characterizing and Generalizing Shannon
Entropy.''} \emph{Journal of Political Economy} 130 (6): 1676--1715.

\bibitem[\citeproctext]{ref-cayton2008algorithms}
Cayton, Lawrence. 2008. {``Algorithms for Manifold Learning.''}
eScholarship, University of California.

\bibitem[\citeproctext]{ref-chen2023information}
Chen, Sihan, Richard Futrell, and Kyle Mahowald. 2023. {``An
Information-Theoretic Approach to the Typology of Spatial
Demonstratives.''} \emph{Cognition} 240: 105505.

\bibitem[\citeproctext]{ref-danielsson2022artificial}
Danielsson, Jon, Robert Macrae, and Andreas Uthemann. 2022.
{``Artificial Intelligence and Systemic Risk.''} \emph{Journal of
Banking \& Finance} 140: 106290.

\bibitem[\citeproctext]{ref-davis2015commonsense}
Davis, Ernest, and Gary Marcus. 2015. {``Commonsense Reasoning and
Commonsense Knowledge in Artificial Intelligence.''}
\emph{Communications of the ACM} 58 (9): 92--103.

\bibitem[\citeproctext]{ref-dean2023experimental}
Dean, Mark, and Nathaniel Neligh. 2023. {``Experimental Tests of
Rational Inattention.''} \emph{Journal of Political Economy} 131 (12):
3415--61.

\bibitem[\citeproctext]{ref-debreu1984economic}
Debreu, Gerard. 1984. {``Economic Theory in the Mathematical Mode.''}
\emph{The American Economic Review} 74 (3): 267--78.

\bibitem[\citeproctext]{ref-dziri2024faith}
Dziri, Nouha, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang,
Bill Yuchen Lin, Sean Welleck, et al. 2024. {``Faith and Fate: Limits of
Transformers on Compositionality.''} \emph{Advances in Neural
Information Processing Systems} 36.

\bibitem[\citeproctext]{ref-ferrario2022eliciting}
Ferrario, Beatrice, and Stefanie Stantcheva. 2022. {``Eliciting People's
First-Order Concerns: Text Analysis of Open-Ended Survey Questions.''}
In \emph{AEA Papers and Proceedings}, 112:163--69. American Economic
Association 2014 Broadway, Suite 305, Nashville, TN 37203.

\bibitem[\citeproctext]{ref-mighty1994fallen}
Gans, Joshua S., and George B. Shepherd. 1994. {``How Are the Mighty
Fallen: Rejected Classic Articles by Leading Economists.''}
\emph{Journal of Economic Perspectives} 8 (1): 165--79.
\url{https://doi.org/10.1257/jep.8.1.165}.

\bibitem[\citeproctext]{ref-gennaioli2010comes}
Gennaioli, Nicola, and Andrei Shleifer. 2010. {``What Comes to Mind.''}
\emph{The Quarterly Journal of Economics} 125 (4): 1399--1433.

\bibitem[\citeproctext]{ref-rationality2023gilboa}
Gilboa, Itzhak, Stefania Minardi, and Fan Wang. 2023. {``{Schumpeter
Lecture 2023: Rationality and Zero Risk}.''} \emph{Journal of the
European Economic Association} 22 (1): 1--33.
\url{https://doi.org/10.1093/jeea/jvad071}.

\bibitem[\citeproctext]{ref-gilboa2014analogies}
Gilboa, Itzhak, Andrew Postlewaite, Larry Samuelson, and David
Schmeidler. 2014. {``{Economic Models as Analogies}.''} \emph{The
Economic Journal} 124 (578): F513--33.
\url{https://doi.org/10.1111/ecoj.12128}.

\bibitem[\citeproctext]{ref-theory2022gilboa}
---------. 2022. {``Economic Theory: Economics, Methods and
Methodology.''} \emph{Revue Économique} 73 (6): pp. 897--920.
\url{https://www.jstor.org/stable/48714515}.

\bibitem[\citeproctext]{ref-GILBOA20101757}
Gilboa, Itzhak, and David Schmeidler. 2010. {``Simplicity and
Likelihood: An Axiomatic Approach.''} \emph{Journal of Economic Theory}
145 (5): 1757--75.
https://doi.org/\url{https://doi.org/10.1016/j.jet.2010.03.010}.

\bibitem[\citeproctext]{ref-HASSABIS2017245}
Hassabis, Demis, Dharshan Kumaran, Christopher Summerfield, and Matthew
Botvinick. 2017. {``Neuroscience-Inspired Artificial Intelligence.''}
\emph{Neuron} 95 (2): 245--58.
https://doi.org/\url{https://doi.org/10.1016/j.neuron.2017.06.011}.

\bibitem[\citeproctext]{ref-hebert2021neighborhood}
Hébert, Benjamin, and Michael Woodford. 2021. {``Neighborhood-Based
Information Costs.''} \emph{American Economic Review} 111 (10):
3225--55. \url{https://doi.org/10.1257/aer.20200154}.

\bibitem[\citeproctext]{ref-heckman2023econometric}
Heckman, James J, and Rodrigo Pinto. 2023. {``Econometric Causality: The
Central Role of Thought Experiments.''} National Bureau of Economic
Research.

\bibitem[\citeproctext]{ref-heckman2015causal}
Heckman, James, and Rodrigo Pinto. 2015. {``Causal Analysis After
Haavelmo.''} \emph{Econometric Theory} 31 (1): 115--51.

\bibitem[\citeproctext]{ref-kim2019minimax}
Kim, Jisu, Alessandro Rinaldo, and Larry Wasserman. 2019. {``Minimax
Rates for Estimating the Dimension of a Manifold.''} \emph{Journal of
Computational Geometry} 10 (1).
\url{https://doi.org/10.20382/JOCG.V10I1A3}.

\bibitem[\citeproctext]{ref-kleinberg2000small}
Kleinberg, Jon. 2000. {``The Small-World Phenomenon: An Algorithmic
Perspective.''} In \emph{Proceedings of the Thirty-Second Annual ACM
Symposium on Theory of Computing}, 163--70.

\bibitem[\citeproctext]{ref-kleinberg1999authoritative}
Kleinberg, Jon M. 1999. {``Authoritative Sources in a Hyperlinked
Environment.''} \emph{Journal of the ACM (JACM)} 46 (5): 604--32.

\bibitem[\citeproctext]{ref-kleinberg2023inversion}
Kleinberg, Jon, Jens Ludwig, Sendhil Mullainathan, and Manish Raghavan.
n.d. {``The Inversion Problem: Why Algorithms Should Infer Mental State
and Not Just Predict Behavior.''} \emph{Perspectives on Psychological
Science}, 17456916231212138.

\bibitem[\citeproctext]{ref-korinek2023gen}
Korinek, Anton. 2023. {``Generative AI for Economic Research: Use Cases
and Implications for Economists.''} \emph{Journal of Economic
Literature} 61 (4): 1281--1317.
\url{https://doi.org/10.1257/jel.20231736}.

\bibitem[\citeproctext]{ref-laban2023you}
Laban, Philippe, Lidiya Murakhovs' ka, Caiming Xiong, and Chien-Sheng
Wu. 2023. {``Are You Sure? Challenging Llms Leads to Performance Drops
in the Flipflop Experiment.''} \emph{arXiv Preprint arXiv:2311.08596}.

\bibitem[\citeproctext]{ref-larochelle2010learning}
Larochelle, Hugo, and Geoffrey E Hinton. 2010. {``Learning to Combine
Foveal Glimpses with a Third-Order Boltzmann Machine.''} \emph{Advances
in Neural Information Processing Systems} 23.

\bibitem[\citeproctext]{ref-lecun2022path}
LeCun, Yann. 2022. {``A Path Towards Autonomous Machine Intelligence
Version 0.9. 2, 2022-06-27.''} \emph{Open Review} 62.

\bibitem[\citeproctext]{ref-lee2023geometric}
Lee, Yonghyeon. 2023. {``A Geometric Perspective on Autoencoders.''}
\emph{arXiv Preprint arXiv:2309.08247}.

\bibitem[\citeproctext]{ref-levina2004maximum}
Levina, Elizaveta, and Peter Bickel. 2004. {``Maximum Likelihood
Estimation of Intrinsic Dimension.''} \emph{Advances in Neural
Information Processing Systems} 17.

\bibitem[\citeproctext]{ref-lewbel2019identification}
Lewbel, Arthur. 2019. {``The Identification Zoo: Meanings of
Identification in Econometrics.''} \emph{Journal of Economic Literature}
57 (4): 835--903.

\bibitem[\citeproctext]{ref-loh2014efficient}
Loh, Lay Kuan, and Mihovil Bartulovic. 2014. {``Efficient Coding
Hypothesis and an Introduction to Information Theory.''} \emph{Mimeo}.

\bibitem[\citeproctext]{ref-lucas1976econometric}
Lucas, Robert E. 1976. {``Econometric Policy Evaluation: A Critique.''}
\emph{Journal of Monetary Economics} 1 (2): 19--46.

\bibitem[\citeproctext]{ref-ludwig2024machine}
Ludwig, Jens, and Sendhil Mullainathan. 2024. {``{Machine Learning as a
Tool for Hypothesis Generation*}.''} \emph{The Quarterly Journal of
Economics}, January, qjad055. \url{https://doi.org/10.1093/qje/qjad055}.

\bibitem[\citeproctext]{ref-mahowald2023dissociating}
Mahowald, Kyle, Anna A. Ivanova, Idan A. Blank, Nancy Kanwisher, Joshua
B. Tenenbaum, and Evelina Fedorenko. 2023. {``Dissociating Language and
Thought in Large Language Models.''}
\url{https://arxiv.org/abs/2301.06627}.

\bibitem[\citeproctext]{ref-marschak1944random}
Marschak, Jacob, and William H Andrews. 1944. {``Random Simultaneous
Equations and the Theory of Production.''} \emph{Econometrica, Journal
of the Econometric Society}, 143--205.

\bibitem[\citeproctext]{ref-mei2024turing}
Mei, Qiaozhu, Yutong Xie, Walter Yuan, and Matthew O. Jackson. 2024.
{``A Turing Test of Whether AI Chatbots Are Behaviorally Similar to
Humans.''} \emph{Proceedings of the National Academy of Sciences} 121
(9): e2313925121. \url{https://doi.org/10.1073/pnas.2313925121}.

\bibitem[\citeproctext]{ref-mitchell2023debate}
Mitchell, Melanie, and David C. Krakauer. 2023. {``The Debate over
Understanding in AI's Large Language Models.''} \emph{Proceedings of the
National Academy of Sciences} 120 (13): e2215907120.
\url{https://doi.org/10.1073/pnas.2215907120}.

\bibitem[\citeproctext]{ref-mnih2014recurrent}
Mnih, Volodymyr, Nicolas Heess, Alex Graves, et al. 2014. {``Recurrent
Models of Visual Attention.''} \emph{Advances in Neural Information
Processing Systems} 27.

\bibitem[\citeproctext]{ref-MOLINARI2020355}
Molinari, Francesca. 2020. {``Chapter 5 - Microeconometrics with Partial
Identification⋆⋆i Thank Don Andrews, Isaiah Andrews, Levon Barseghyan,
Federico Bugni, Ivan Canay, Joachim Freyberger, Hiroaki Kaido, Toru
Kitagawa, Chuck Manski, Rosa Matzkin, Ilya Molchanov, Áureo de Paula,
Jack Porter, Seth Richards-Shubik, Adam Rosen, Shuyang Sheng, Jörg
Stoye, Elie Tamer, Matthew Thirkettle, and Participants to the 2017
Handbook of Econometrics Conference, for Helpful Comments, and the
National Science Foundation for Financial Support Through Grants
SES-1824375 and SES-1824448. I Am Grateful to Louis Liu and Yibo Sun for
Research Assistance Supported by the Robert s. Hatfield Fund for
Economic Education at Cornell University. Part of This Research Was
Carried Out During My Sabbatical Leave at the Department of Economics at
Duke University, Whose Hospitality i Gratefully Acknowledge.''} In
\emph{Handbook of Econometrics, Volume 7A}, edited by Steven N. Durlauf,
Lars Peter Hansen, James J. Heckman, and Rosa L. Matzkin, 7:355--486.
Handbook of Econometrics. Elsevier.
https://doi.org/\url{https://doi.org/10.1016/bs.hoe.2020.05.002}.

\bibitem[\citeproctext]{ref-olshausen1996natural}
Olshausen, Bruno A, and David J Field. 1996. {``Natural Image Statistics
and Efficient Coding.''} \emph{Network: Computation in Neural Systems} 7
(2): 333.

\bibitem[\citeproctext]{ref-parkes2015economic}
Parkes, David C, and Michael P Wellman. 2015. {``Economic Reasoning and
Artificial Intelligence.''} \emph{Science} 349 (6245): 267--72.

\bibitem[\citeproctext]{ref-perez2024testing}
Perez-Cruz, Fernando, and Hyun Song Shin. 2024. {``Testing the Cognitive
Limits of Large Language Models.''} Bank for International Settlements.

\bibitem[\citeproctext]{ref-intrinsic2021}
Pope, Phillip, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom
Goldstein. 2021. {``The Intrinsic Dimension of Images and Its Impact on
Learning.''} \emph{CoRR} abs/2104.08894.
\url{https://arxiv.org/abs/2104.08894}.

\bibitem[\citeproctext]{ref-prystawski2024think}
Prystawski, Ben, Michael Li, and Noah Goodman. 2024. {``Why Think Step
by Step? Reasoning Emerges from the Locality of Experience.''}
\emph{Advances in Neural Information Processing Systems} 36.

\bibitem[\citeproctext]{ref-robbins1932essay}
Robbins, Lionel. 1932. \emph{An Essay on the Nature and Significance of
Economic Science}. Macmillan; Co., Limited.

\bibitem[\citeproctext]{ref-schaeffer2024emergent}
Schaeffer, Rylan, Brando Miranda, and Sanmi Koyejo. 2024. {``Are
Emergent Abilities of Large Language Models a Mirage?''} \emph{Advances
in Neural Information Processing Systems} 36.

\bibitem[\citeproctext]{ref-searle1980minds}
Searle, John R. 1980. {``Minds, Brains, and Programs.''}
\emph{Behavioral and Brain Sciences} 3 (3): 417--24.

\bibitem[\citeproctext]{ref-shannon1948mathematical}
Shannon, Claude Elwood. 1948. {``A Mathematical Theory of
Communication.''} \emph{The Bell System Technical Journal} 27 (3):
379--423.

\bibitem[\citeproctext]{ref-shleifer2012psychologists}
Shleifer, Andrei. 2012. {``Psychologists at the Gate: A Review of Daniel
Kahneman's Thinking, Fast and Slow.''} \emph{Journal of Economic
Literature} 50 (4): 1080--91.

\bibitem[\citeproctext]{ref-SIMS2003665}
Sims, Christopher A. 2003. {``Implications of Rational Inattention.''}
\emph{Journal of Monetary Economics} 50 (3): 665--90.
https://doi.org/\url{https://doi.org/10.1016/S0304-3932(03)00029-1}.

\bibitem[\citeproctext]{ref-stantcheva2023run}
Stantcheva, Stefanie. 2023. {``How to Run Surveys: A Guide to Creating
Your Own Identifying Variation and Revealing the Invisible.''}
\emph{Annual Review of Economics} 15: 205--34.

\bibitem[\citeproctext]{ref-storks2020recent}
Storks, Shane, Qiaozi Gao, and Joyce Y. Chai. 2020. {``Recent Advances
in Natural Language Inference: A Survey of Benchmarks, Resources, and
Approaches.''} \url{https://arxiv.org/abs/1904.01172}.

\bibitem[\citeproctext]{ref-tenenbaum2000global}
Tenenbaum, Joshua B, Vin de Silva, and John C Langford. 2000. {``A
Global Geometric Framework for Nonlinear Dimensionality Reduction.''}
\emph{Science} 290 (5500): 2319--23.

\bibitem[\citeproctext]{ref-vaswani2023attention}
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023.
{``Attention Is All You Need.''} \url{https://arxiv.org/abs/1706.03762}.

\bibitem[\citeproctext]{ref-wei2022emergent}
Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
Sebastian Borgeaud, Dani Yogatama, et al. 2022. {``Emergent Abilities of
Large Language Models.''} \emph{arXiv Preprint arXiv:2206.07682}.

\bibitem[\citeproctext]{ref-wei2022chain}
Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed
Chi, Quoc V Le, Denny Zhou, et al. 2022. {``Chain-of-Thought Prompting
Elicits Reasoning in Large Language Models.''} \emph{Advances in Neural
Information Processing Systems} 35: 24824--37.

\bibitem[\citeproctext]{ref-xie2023ask}
Xie, Qiming, Zengzhi Wang, Yi Feng, and Rui Xia. 2023. {``Ask Again,
Then Fail: Large Language Models' Vacillations in Judgement.''}
\emph{arXiv Preprint arXiv:2310.02174}.

\bibitem[\citeproctext]{ref-yang2023harnessing}
Yang, Jingfeng, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng,
Haoming Jiang, Bing Yin, and Xia Hu. 2023. {``Harnessing the Power of
LLMs in Practice: A Survey on Chatgpt and Beyond.''} \emph{arXiv
Preprint arXiv:2304.13712}.

\bibitem[\citeproctext]{ref-zaslavsky2018efficient}
Zaslavsky, Noga, Charles Kemp, Terry Regier, and Naftali Tishby. 2018.
{``Efficient Compression in Color Naming and Its Evolution.''}
\emph{Proceedings of the National Academy of Sciences} 115 (31):
7937--42.

\bibitem[\citeproctext]{ref-zellers2019hellaswag}
Zellers, Rowan, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
2019. {``Hellaswag: Can a Machine Really Finish Your Sentence?''}
\emph{arXiv Preprint arXiv:1905.07830}.

\end{CSLReferences}



\end{document}
