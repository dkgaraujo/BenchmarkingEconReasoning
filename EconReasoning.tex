% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage[noblocks]
{authblk}
\renewcommand*{\Authsep}{, }
\renewcommand*{\Authand}{, }
\renewcommand*{\Authands}{, }
\renewcommand\Affilfont{\small}
\DeclareMathOperator*{\argmin}{arg\,min}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Measuring the economic reasoning abilities of language models},
  pdfauthor={Douglas K. G. Araujo},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Measuring the economic reasoning abilities of language
models\thanks{This work represents my opinion and not necessarily that
of the BIS.}}


  \author{Douglas K. G. Araujo}
            \affil{%
                  Bank for International
Settlements, douglas.araujo@bis.org
              }
      
\date{}
\begin{document}
\maketitle
\begin{abstract}
Economic reasoning is represented as a state space function.
\end{abstract}

\section{Introduction}\label{introduction}

Language models (LMs), in particular those classified as generative
artificial intelligence (gen AI), are finding increasing uses in finance
and economics. These models are usually tested for their ability to
reason, and seem to do well: for example, OpenAI's GPT-4 boats more than
80\% correct results in academic and professional micro- and
macroeconomics tests (Achiam et al. (2023)). Still, even such advanced
models can fail miserably. Perez-Cruz and Shin (2024) demonstrate how
the same model can correctly solve a logical puzzle requiring reasoning
about higher order knowledge, only to fail when irrelevant details are
changed. Building on results such as this and other examples that
clearly illustrate the limits of rationality assumptions on LMs, this
work discusses how to systematically measure \emph{economic} reasoning,
combining literatures on economic thought and on computer science about
gen AI benchmarking.

The main intuition of this work is to combine a number of building
blocks of evaluation. First, adversarial filtering as in HellaSwag.
Second, evolving test set based on newly published academic work. Third,
perturbations in the spirit of Alzahrani et al. (2024). And fourth, a
mathematical adjustment that takes into account performance across
perturbations, penalising results that vary with \ldots.

This benchmark evaluation addresses a poignant issue for the economics
profession: the lack of publicly available data about how these
benchmarks are created and any, and toasted.

\section{A model of economic
reasoning}\label{a-model-of-economic-reasoning}

The result from existing benchmarks is largely, if not completely,
directly related to the number of questions correctly answered. However,
this measures only the model's ability to answer correctly, \emph{not
necessarily} its reasoning capabilities. The latter are part of a latent
state space sitting between the input prompt and the answer. More
concretely, for an input prompt \(X\), which includes a question and any
necessary explicit information, the language model is a function
\(\mathbf{M}\) that maps it to a given response:
\(\mathbf{M} : X \to y\). In order to show that it is done by reasoning,
we need tests (and more specifically, measurements) that convey some
information about the inner workings of this function.

\subsection{Reasoning as an abstract of the
input}\label{reasoning-as-an-abstract-of-the-input}

\begin{itemize}
\item
  Input prompt \(X\)
\item
  Transformed into \(g(X)\), a state space function that maps it to its
  abstract fundamentals (similar to manifold learning)
\item
  Result based on \(g(X)\).
\end{itemize}

\subsection{A (very) simple model}\label{a-very-simple-model}

This section builds on the intuition that in true reasoning, the result
should be robust to minute perturbations, ie the model is a constant
function over the domain of the input. Formally, both
\(\mathbf{M}(X) = y\) and \(\mathbf{M}(X + \epsilon) = y\) for an
infinitesimal \(\epsilon\). This implies the derivative with respect to
the input prompt is zero. Using as an approachable example the simplest
possible neural network, the logistic regression
\(\mathbf{N}(x) = \sigma(Wx + b)\), such robustness further implies that
\(\frac{d\mathbf{N}}{d x} = \sigma(Wx + b)(1-\sigma(Wx + b))W = 0\).
Because \(W\) cannot be a zero vector in a functioning network that is
responsive to its inputs and \(\sigma(Wx + b)(1-\sigma(Wx + b)) = 0\)
has no solution because neither term is 0 or 1 in a sigmoid function
with finite inputs, the neural network cannot be a constant function.
This extremely simplified example does not bode well for the robustness
of results given small perturbations in the input prompt.

\section{Reasoning benchmarks in other
fields}\label{reasoning-benchmarks-in-other-fields}

\begin{itemize}
\item
  Math
\item
  Medical
\item
  Biologia
\end{itemize}

\section{A model of reasoning}\label{a-model-of-reasoning}

This section develops a model of reasoning that fits naturally into both
natural and artificial LMs. It will serve as the basis for the
subsequent analyses and empirical creation of a reasoning benchmark.

Let a sentence \(\mathbf{S} = (\theta_1, \theta_2, \theta_3, ...)\) be a
sequence of tokens-location tuples \(\theta_x = (\tau, x)\), with each
\(\tau \in \mathbf{V}\) belonging to a vocabulary \(\mathbf{V}\) and
\(x \in \mathbb{N}^{d_{\text{model}}}\).\footnote{The location is
  important because it helps define meaning, along with the actual
  letter (more generally, symbol) content of th token. Note that in this
  paper, white spaces are abstracted away for expositional simplicity.}
Create a function \(\pi_{i, C} : \theta, \mathbf{S} \to \{-1, 0, 1\}\)
that maps each token into one of three possibilities: the token's
information can be considered a adversarial (-1), irrelevant (0) or
relevant (1) with respect to the likelihood of individual (or LM) \(i\)
uttering another sentence C. For example, take the following quote from
the character Barf in the 1987 movie Spaceballs, organised as two
sentences ``I'm a mog. Half man, half dog.'' and ``I'm my own best
friend.'' With word-level tokenisation,
\(\mathbf{S} = \{("\text{I'm}", 1), ("\text{a}", 2), ("\text{mog}", 3), ("\text{.}", 4), ("\text{Half}", 5), ("\text{man}", 6), ("\text{,}", 7), ("\text{half}", 8), ("\text{dog}", 9), ("\text{.}", 10)\}\)
and \(\mathbf{C}\) is similarly broken down. This example illustrates
that even when there is not a logical connection grounded in truth,
tokens in one sentence - even those made up like ``mog'', can have a
bearing on the likelihood of tokens appearing in another sentence. This
likelihood can differ depending on the location of the token, which also
allows for situations where repeteating of a word \(\tau\) is meant to
convey different meaning. Another feature of this example is that all
\(\pi_{\text{Barf}, C}(\theta) = 1 \forall \theta \in \mathbf{S}\). In
the alternative sentence ``I'm a mog. Half man, half dog. I am alive.'',
the new component is obviously irrelevant for \(\mathbf{C}\):
\(\prod_{x \in [10, 14]} \pi_{\text{Barf}, C}(\theta_x) = 0\).

This exposition is important to delve into the reasoning aspect,
entirely organised by function \(\pi\). Since \(\pi_{i, C}\) measures
how informative a token is for individual \(i\)'s \(\mathbf{C}\), it
constitutes the first aspect of reasoning: to recognise when a token is
adversarial, irrelevant or relevant. This step is necessary before the
application of any logical rules \(\mathcal{l} \in \mathcal{L}\) on the
weighted token, \(\pi_{i, C}(\theta_x) \theta_x\). The exact
underpinnings of these logical rules are beyond the scope of this work -
it can be approximated by a possibly non-linear function, \(g\). What
suffices in this work is to say that reasoning \emph{depends} on
correctly classifying the tokens: all relevant tokens must be so
identified, lest they be either ignored as the irrelevant ones or taken
with the opposite meaning. Similarly, if all relevant tokens are indeed
diagnosed correctly but other tokens are also diagnosed as relevant when
they are not, then this will cause problems for the correct reasoning.
In other words, a first precondition for reasoning is to have a low
categorical cross-entropy loss. Intuitively, a pre-condition of
reasoning is to correctly interpret the inputs.

But what determines \(\pi_{i, C}\)? A combination of knowledges and
rationales or logical relationships.

Knowledges: linguistic knowledge, common knowledge and commonsense
knowledge

Rationales: reasoning from logic

Armed with the sentence-level categorical cross-entropy, the individual
can establish chains of thought that will finally lead to reasoning.
Again, for simplicity, the exact function is not discussed here, other
than that it is a potentially simple or complex way to interact. What is
important is to add the categorical cross-entropy to the estimation
equation.

\section{Reasoning about economics}\label{reasoning-about-economics}

The model above allows us to estimate reasoning while also breaking down
some of its components to better understand them. For example, we can
estimate any errors in reasoning into an issue with
\textbf{interpretation}, \textbf{knowledge} and \textbf{logical
thinking}. The empirical estimation follows.

\section{Empirical estimation}\label{empirical-estimation}

Each \emph{task} \(\theta \in \Theta\) can be asked in various different
ways, each one being called a \emph{question} \(q \in \theta\).
Questions vary with respect to their adversarial aspect. Following
Alzahrani et al. (2024), the variations \(q\) are:

\begin{itemize}
\item
  random choice order
\item
  biased choice order
\item
  uncommon answer choice symbols
\item
  common but unordered answer choice symbols
\end{itemize}

In practice, each task has hundreds of different \(q\). The variation in
response between the questions within each task will comprise the
evaluation of the actual reasoning capabilities.

In addition, two more tests are added: the changing of irrelevant detail
(a la Perez-Cruz and Shin (2024)) and random word repetition as if it
were a typo.

\section{Operational characteristics}\label{operational-characteristics}

\begin{itemize}
\tightlist
\item
  avoid becoming part of training data
\end{itemize}

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-achiam2023gpt}
Achiam, Josh, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
Florencia Leoni Aleman, Diogo Almeida, et al. 2023. {``Gpt-4 Technical
Report.''} \emph{arXiv Preprint arXiv:2303.08774}.

\bibitem[\citeproctext]{ref-alzahrani2024benchmarks}
Alzahrani, Norah, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan
Alrashed, Shaykhah Alsubaie, Yusef Almushaykeh, Faisal Mirza, et al.
2024. {``When Benchmarks Are Targets: Revealing the Sensitivity of Large
Language Model Leaderboards.''} \emph{arXiv Preprint arXiv:2402.01781}.

\bibitem[\citeproctext]{ref-perez2024testing}
Perez-Cruz, Fernando, and Hyun Song Shin. 2024. {``Testing the Cognitive
Limits of Large Language Models.''} Bank for International Settlements.

\end{CSLReferences}



\end{document}
