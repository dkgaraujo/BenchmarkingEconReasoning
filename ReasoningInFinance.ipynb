{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Imperfect reasoning abilities of artificial intelligence models and asset prices (Early stage work)\"\n",
    "author:\n",
    "    name:  \"Douglas K. G. Araujo\"\n",
    "    email: \"douglas.araujo@bis.org\"\n",
    "    affiliations:  \"Bank for International Settlements\"\n",
    "thanks: \"This work represents my opinion and not necessarily that of the BIS.\"\n",
    "abstract: \"Market participants choose the level of costly deployment of artificial intelligence models, such as large language models, to better extract signal about fundamentals from a noisy and multidimensional common data space. The monotonically increasing technology frontier evolves over time but actual adoption depends on the use of scarce resources that influence costs, mimicking constraints in human resources and parallel computing. Participants make decisions based on a 'double global game' with simultaneous higher-order beliefs about asset prices and technology adoption by peers. Similar to the traditional case with assets, participants overinvest in AI because they think others might be doing so, etc. But while this overinvestment in AI reduces the noise about fundamentals, the higher-order beliefs that everyone else also observes lower noise ends up leading to the same theme of overinvestment in the asset as well. If the technology envelope does not include a properly reasoning AI model, which is able to robustly ignore noise about fundamentals, the equilibrium outcome is a self-reinforcing overinvestment both in the asset and in the AI-related resources. In contrast, the emergence of a reasoning model brings the noise to zero and agents coordinate perfectly. JEL codes: C45, C69, C88, C59.\"\n",
    "format:\n",
    "    pdf:\n",
    "        keep-tex: true\n",
    "        template-partials: \n",
    "            - title.tex\n",
    "        include-in-header:\n",
    "            text: |\n",
    "                \\usepackage{algpseudocode}\n",
    "                \\usepackage{algorithm}\n",
    "                \\usepackage[noblocks]\n",
    "                {authblk}\n",
    "                \\renewcommand*{\\Authsep}{, }\n",
    "                \\renewcommand*{\\Authand}{, }\n",
    "                \\renewcommand*{\\Authands}{, }\n",
    "                \\renewcommand\\Affilfont{\\small}\n",
    "                \\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "        number-sections: true\n",
    "bibliography: \"ref.bib\"\n",
    "execute:\n",
    "    echo: false\n",
    "    warning: false\n",
    "    error: false\n",
    "    cache: true\n",
    "    freeze: auto\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increasing capabilities of large language models (LLM) and other artificial intelligence (AI) models create expectations that they can be useful for forecasting and trading (eg, @lopez2023can). But if all market participants expect others to improve their own signal-to-noise ratio by adopting a common technology, a coordination situation with higher-order beliefs similar to the canonical global games arises, in conjunction with the original coordination issue. This paper explores the equilibrium outcome when such a technology evolves according to availability of resources that are also deployed at a cost by market participants to improve their signal.\n",
    "\n",
    "\n",
    "Market participants choose the level of costly deployment of artificial intelligence models, such as large language models, to better extract signal about fundamentals from a noisy and multidimensional common data space. The monotonically increasing technology frontier evolves over time but actual adoption depends on the use of scarce resources that influence costs, mimicking constraints in human resources and parallel computing. Participants make decisions based on a 'double global game' with simultaneous higher-order beliefs about asset prices and technology adoption by peers. Similar to the traditional case with assets, participants overinvest in AI because they think others might be doing so, etc. But while this overinvestment in AI reduces the noise about fundamentals, the higher-order beliefs that everyone else also observes lower noise ends up leading to the same theme of overinvestment in the asset as well. If the technology envelope does not include a properly reasoning AI model, which is able to robustly ignore noise about fundamentals, the equilibrium outcome is a self-reinforcing overinvestment both in the asset and in the AI-related resources. In contrast, the emergence of a reasoning model brings the noise to zero and agents coordinate perfectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work relates to the literatures on \n",
    "\n",
    "**Coordination with information acquisition**. @angeletos2016incomplete, @szkup2015informationgg, @szkup2015informationtr, @szkup2021information. @reshidi2021individual study the individual and collective information acquisition. Technology adoption canonical model @techadoption. Private aquisition of information (processing), @hellwig2009knowing and @colombo2014information. In contrast with that literature, here the AI technology frontier also develops endogenously and responds (negatively) to the resource take-up from technology adoption...\n",
    "\n",
    "**AI in finance, including risks.** @danielsson2022artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equilibrium @frankel2003equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@szkup2020multiplier show that only the direct effect is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv_gingado')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "977c2d9435ad3a481cf1bbece8d5ecb19e078de55648a0a0bad32b79c2e18340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
